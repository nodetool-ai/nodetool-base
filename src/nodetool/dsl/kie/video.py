# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class GrokImagineImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from images using xAI's Grok Imagine model via Kie.ai.

        kie, grok, xai, video generation, ai, image-to-video, multimodal

        Grok Imagine transforms images into videos using xAI's
        multimodal generation capabilities.
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.GrokImagineImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text guide for the animation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The source image to animate.",
    )
    duration: nodetool.nodes.kie.video.GrokImagineImageToVideo.Duration = Field(
        default=nodetool.nodes.kie.video.GrokImagineImageToVideo.Duration.MEDIUM,
        description="The duration tier of the video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.GrokImagineImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class GrokImagineTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text using xAI's Grok Imagine model via Kie.ai.

        kie, grok, xai, video generation, ai, text-to-video, multimodal

        Grok Imagine generates videos from text prompts using xAI's
        multimodal generation capabilities.
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.GrokImagineTextToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.GrokImagineTextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )
    resolution: nodetool.nodes.kie.video.GrokImagineTextToVideo.Resolution = Field(
        default=nodetool.nodes.kie.video.GrokImagineTextToVideo.Resolution.R1080P,
        description="The resolution of the video.",
    )
    duration: nodetool.nodes.kie.video.GrokImagineTextToVideo.Duration = Field(
        default=nodetool.nodes.kie.video.GrokImagineTextToVideo.Duration.MEDIUM,
        description="The duration tier of the video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.GrokImagineTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class HailuoImageToVideoPro(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from images using MiniMax's Hailuo 2.3 Pro model via Kie.ai.

        kie, hailuo, minimax, video generation, ai, image-to-video, pro

        Hailuo 2.3 Pro offers the highest quality image-to-video generation with
        realistic motion, detailed textures, and cinematic quality.
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.HailuoImageToVideoPro.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.HailuoImageToVideoPro.Resolution
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The reference image to animate into a video.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text to guide the video generation.",
    )
    duration: nodetool.nodes.kie.video.HailuoImageToVideoPro.Duration = Field(
        default=nodetool.nodes.kie.video.HailuoImageToVideoPro.Duration.D6,
        description="The duration of the video in seconds. 10s is not supported for 1080p.",
    )
    resolution: nodetool.nodes.kie.video.HailuoImageToVideoPro.Resolution = Field(
        default=nodetool.nodes.kie.video.HailuoImageToVideoPro.Resolution.R768P,
        description="Video resolution.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.HailuoImageToVideoPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class HailuoImageToVideoStandard(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from images using MiniMax's Hailuo 2.3 Standard model via Kie.ai.

        kie, hailuo, minimax, video generation, ai, image-to-video, standard, fast

        Hailuo 2.3 Standard offers efficient image-to-video generation with good quality
        and faster processing times for practical use cases.
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.HailuoImageToVideoStandard.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.HailuoImageToVideoStandard.Resolution
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The reference image to animate into a video.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text to guide the video generation.",
    )
    duration: nodetool.nodes.kie.video.HailuoImageToVideoStandard.Duration = Field(
        default=nodetool.nodes.kie.video.HailuoImageToVideoStandard.Duration.D6,
        description="The duration of the video in seconds. 10s is not supported for 1080p.",
    )
    resolution: nodetool.nodes.kie.video.HailuoImageToVideoStandard.Resolution = Field(
        default=nodetool.nodes.kie.video.HailuoImageToVideoStandard.Resolution.R768P,
        description="Video resolution.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.HailuoImageToVideoStandard

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class HailuoTextToVideoPro(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text using MiniMax's Hailuo 2.3 Pro model via Kie.ai.

        kie, hailuo, minimax, video generation, ai, text-to-video, pro

        Hailuo 2.3 Pro offers the highest quality text-to-video generation with
        realistic motion, detailed textures, and cinematic quality.
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.HailuoTextToVideoPro.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.HailuoTextToVideoPro.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )
    duration: nodetool.nodes.kie.video.HailuoTextToVideoPro.Duration = Field(
        default=nodetool.nodes.kie.video.HailuoTextToVideoPro.Duration.D6,
        description="The duration of the video in seconds. 10s is not supported for 1080p.",
    )
    resolution: nodetool.nodes.kie.video.HailuoTextToVideoPro.Resolution = Field(
        default=nodetool.nodes.kie.video.HailuoTextToVideoPro.Resolution.R768P,
        description="Video resolution.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.HailuoTextToVideoPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class HailuoTextToVideoStandard(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text using MiniMax's Hailuo 2.3 Standard model via Kie.ai.

        kie, hailuo, minimax, video generation, ai, text-to-video, standard, fast
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.HailuoTextToVideoStandard.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.HailuoTextToVideoStandard.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )
    duration: nodetool.nodes.kie.video.HailuoTextToVideoStandard.Duration = Field(
        default=nodetool.nodes.kie.video.HailuoTextToVideoStandard.Duration.D6,
        description="The duration of the video in seconds. 10s is not supported for 1080p.",
    )
    resolution: nodetool.nodes.kie.video.HailuoTextToVideoStandard.Resolution = Field(
        default=nodetool.nodes.kie.video.HailuoTextToVideoStandard.Resolution.R768P,
        description="Video resolution.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.HailuoTextToVideoStandard

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class InfinitalkV1(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos using Infinitalk v1 (image-to-video) via Kie.ai.
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.InfinitalkV1.Resolution

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text guide for the video generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The source image.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The source audio track.",
    )
    resolution: nodetool.nodes.kie.video.InfinitalkV1.Resolution = Field(
        default=nodetool.nodes.kie.video.InfinitalkV1.Resolution.R480P,
        description="Video resolution.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.InfinitalkV1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Kling25TurboImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from images using Kuaishou's Kling 2.5 Turbo model via Kie.ai.

        kie, kling, kuaishou, video generation, ai, image-to-video, turbo

        Transforms a static image into a dynamic video while preserving
        visual style, colors, lighting, and texture.
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Kling25TurboImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Text description to guide the video generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The source image to animate.",
    )
    tail_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Tail frame image for the video (optional).",
    )
    duration: nodetool.nodes.kie.video.Kling25TurboImageToVideo.Duration = Field(
        default=nodetool.nodes.kie.video.Kling25TurboImageToVideo.Duration.D5,
        description="Video duration in seconds.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Elements to avoid in the video."
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG scale for prompt adherence. Lower values allow more creativity.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Kling25TurboImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Kling25TurboTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text using Kuaishou's Kling 2.5 Turbo model via Kie.ai.

        kie, kling, kuaishou, video generation, ai, text-to-video, turbo

        Kling 2.5 Turbo offers improved prompt adherence, fluid motion,
        consistent artistic styles, and realistic physics simulation.
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Kling25TurboTextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Kling25TurboTextToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )
    duration: nodetool.nodes.kie.video.Kling25TurboTextToVideo.Duration = Field(
        default=nodetool.nodes.kie.video.Kling25TurboTextToVideo.Duration.D5,
        description="Video duration in seconds.",
    )
    aspect_ratio: nodetool.nodes.kie.video.Kling25TurboTextToVideo.AspectRatio = Field(
        default=nodetool.nodes.kie.video.Kling25TurboTextToVideo.AspectRatio.V16_9,
        description="The aspect ratio of the generated video.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Things to avoid in the generated video."
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG scale for prompt adherence. Lower values allow more creativity.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Kling25TurboTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class KlingAIAvatarPro(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate talking avatar videos using Kuaishou's Kling AI via Kie.ai.

        kie, kling, kuaishou, avatar, video generation, ai, talking-head, lip-sync

        Transforms a photo plus audio track into a lip-synced talking avatar video
        with natural-looking speech animation and consistent identity.
    """

    Mode: typing.ClassVar[type] = nodetool.nodes.kie.video.KlingAIAvatarPro.Mode

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The face/character image to animate.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The audio track for lip-syncing.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text to guide emotions and expressions.",
    )
    mode: nodetool.nodes.kie.video.KlingAIAvatarPro.Mode = Field(
        default=nodetool.nodes.kie.video.KlingAIAvatarPro.Mode.STANDARD,
        description="Generation mode: 'standard' or 'pro' for higher quality.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.KlingAIAvatarPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class KlingAIAvatarStandard(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate talking avatar videos using Kuaishou's Kling AI via Kie.ai.

        kie, kling, kuaishou, avatar, video generation, ai, talking-head, lip-sync

        Transforms a photo plus audio track into a lip-synced talking avatar video
        with natural-looking speech animation and consistent identity.
    """

    Mode: typing.ClassVar[type] = nodetool.nodes.kie.video.KlingAIAvatarStandard.Mode

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The face/character image to animate.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The audio track for lip-syncing.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text to guide emotions and expressions.",
    )
    mode: nodetool.nodes.kie.video.KlingAIAvatarStandard.Mode = Field(
        default=nodetool.nodes.kie.video.KlingAIAvatarStandard.Mode.STANDARD,
        description="Generation mode: 'standard' or 'pro' for higher quality.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.KlingAIAvatarStandard

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class KlingImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from images using Kuaishou's Kling 2.6 model via Kie.ai.

        kie, kling, kuaishou, video generation, ai, image-to-video, 2.6

        Transforms static images into dynamic videos with realistic motion
        and temporal consistency while preserving the original visual style.
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text prompt to guide the video generation.",
    )
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="First source image for the video generation.",
    )
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Second source image (optional).",
    )
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Third source image (optional).",
    )
    sound: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to generate sound for the video."
    )
    duration: int | OutputHandle[int] = connect_field(
        default=5, description="Video duration in seconds."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.KlingImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class KlingMotionControl(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos with motion control using Kuaishou's Kling 2.6 model via Kie.ai.

        kie, kling, kuaishou, video generation, ai, motion-control, character-animation, 2.6

        Kling Motion Control generates videos where character actions are guided by a reference video,
        while the visual appearance is based on a reference image. Perfect for character animation
        and motion transfer tasks.
    """

    CharacterOrientation: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.KlingMotionControl.CharacterOrientation
    )
    Mode: typing.ClassVar[type] = nodetool.nodes.kie.video.KlingMotionControl.Mode

    prompt: str | OutputHandle[str] = connect_field(
        default="The cartoon character is dancing.",
        description="A text description of the desired output. Maximum 2500 characters.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Reference image. The characters, backgrounds, and other elements in the generated video are based on this image. Supports .jpg/.jpeg/.png, max 10MB, size needs to be greater than 300px, aspect ratio 2:5 to 5:2.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Reference video. The character actions in the generated video will be consistent with this reference video. Supports .mp4/.mov, max 100MB, 3-30 seconds duration depending on character_orientation.",
    )
    character_orientation: (
        nodetool.nodes.kie.video.KlingMotionControl.CharacterOrientation
    ) = Field(
        default=nodetool.nodes.kie.video.KlingMotionControl.CharacterOrientation.VIDEO,
        description="Generate the orientation of the characters in the video. 'image': same orientation as the person in the picture (max 10s video). 'video': consistent with the orientation of the characters in the video (max 30s video).",
    )
    mode: nodetool.nodes.kie.video.KlingMotionControl.Mode = Field(
        default=nodetool.nodes.kie.video.KlingMotionControl.Mode.R720P,
        description="Output resolution mode. Use '720p' for 720p or '1080p' for 1080p.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.KlingMotionControl

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class KlingTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text using Kuaishou's Kling 2.6 model via Kie.ai.

        kie, kling, kuaishou, video generation, ai, text-to-video, 2.6

        Kling 2.6 produces high-quality videos from text descriptions with
        realistic motion, natural lighting, and cinematic detail.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.KlingTextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.KlingTextToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )
    aspect_ratio: nodetool.nodes.kie.video.KlingTextToVideo.AspectRatio = Field(
        default=nodetool.nodes.kie.video.KlingTextToVideo.AspectRatio.V16_9,
        description="The aspect ratio of the generated video.",
    )
    duration: int | OutputHandle[int] = connect_field(
        default=5, description="Video duration in seconds."
    )
    resolution: nodetool.nodes.kie.video.KlingTextToVideo.Resolution = Field(
        default=nodetool.nodes.kie.video.KlingTextToVideo.Resolution.R768P,
        description="Video resolution.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducible results. Use -1 for random seed.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.KlingTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class LumaModifyVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Modify and enhance videos using Luma's API via Kie.ai.

        kie, luma, video modification, ai, video-editing

        Luma's video modification API allows for sophisticated video editing
        and enhancement capabilities.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.LumaModifyVideo.AspectRatio
    )
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.LumaModifyVideo.Duration

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The source video to modify.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Enhance the video quality and add smooth motion.",
        description="Text prompt describing the modifications to make.",
    )
    aspect_ratio: nodetool.nodes.kie.video.LumaModifyVideo.AspectRatio = Field(
        default=nodetool.nodes.kie.video.LumaModifyVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the output video.",
    )
    duration: nodetool.nodes.kie.video.LumaModifyVideo.Duration = Field(
        default=nodetool.nodes.kie.video.LumaModifyVideo.Duration.D5,
        description="Duration of the modified video segment.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.LumaModifyVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class RunwayAlephVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos using Runway's Aleph model via Kie.ai.

        kie, runway, aleph, video generation, ai, text-to-video

        Aleph is Runway's advanced video generation model offering
        high-quality output with sophisticated motion handling.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.RunwayAlephVideo.AspectRatio
    )
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.RunwayAlephVideo.Duration
    Quality: typing.ClassVar[type] = nodetool.nodes.kie.video.RunwayAlephVideo.Quality

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )
    aspect_ratio: nodetool.nodes.kie.video.RunwayAlephVideo.AspectRatio = Field(
        default=nodetool.nodes.kie.video.RunwayAlephVideo.AspectRatio.V16_9,
        description="The aspect ratio of the generated video. Required for text-to-video generation.",
    )
    duration: nodetool.nodes.kie.video.RunwayAlephVideo.Duration = Field(
        default=nodetool.nodes.kie.video.RunwayAlephVideo.Duration.D5,
        description="Video duration in seconds. If 10-second video is selected, 1080p resolution cannot be used.",
    )
    quality: nodetool.nodes.kie.video.RunwayAlephVideo.Quality = Field(
        default=nodetool.nodes.kie.video.RunwayAlephVideo.Quality.R720P,
        description="Video resolution. If 1080p is selected, 10-second video cannot be generated.",
    )
    water_mark: str | OutputHandle[str] = connect_field(
        default="",
        description="Video watermark text content. An empty string indicates no watermark.",
    )
    call_back_url: str | OutputHandle[str] = connect_field(
        default="",
        description="Optional callback URL to receive task completion updates.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.RunwayAlephVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class RunwayGen3AlphaExtendVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Extend videos using Runway's Gen-3 Alpha model via Kie.ai.

        kie, runway, gen-3, gen3alpha, video generation, ai, video-extension

        Runway Gen-3 Alpha can extend existing videos with additional generated content.
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.RunwayGen3AlphaExtendVideo.Duration
    )
    Quality: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.RunwayGen3AlphaExtendVideo.Quality
    )

    video_url: str | OutputHandle[str] = connect_field(
        default="", description="The source video URL to extend."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Continue the motion naturally with smooth transitions.",
        description="Text prompt to guide the video extension. Maximum length is 1800 characters.",
    )
    duration: nodetool.nodes.kie.video.RunwayGen3AlphaExtendVideo.Duration = Field(
        default=nodetool.nodes.kie.video.RunwayGen3AlphaExtendVideo.Duration.D5,
        description="Duration to extend the video by in seconds. If 10-second extension is selected, 1080p resolution cannot be used.",
    )
    quality: nodetool.nodes.kie.video.RunwayGen3AlphaExtendVideo.Quality = Field(
        default=nodetool.nodes.kie.video.RunwayGen3AlphaExtendVideo.Quality.R720P,
        description="Video resolution. If 1080p is selected, 10-second extension cannot be generated.",
    )
    water_mark: str | OutputHandle[str] = connect_field(
        default="",
        description="Video watermark text content. An empty string indicates no watermark.",
    )
    call_back_url: str | OutputHandle[str] = connect_field(
        default="",
        description="Optional callback URL to receive task completion updates.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.RunwayGen3AlphaExtendVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class RunwayGen3AlphaImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from images using Runway's Gen-3 Alpha model via Kie.ai.

        kie, runway, gen-3, gen3alpha, video generation, ai, image-to-video

        Runway Gen-3 Alpha transforms static images into dynamic videos
        with realistic motion and temporal consistency.
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.RunwayGen3AlphaImageToVideo.Duration
    )
    Quality: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.RunwayGen3AlphaImageToVideo.Quality
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Reference image to base the video on.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text to guide the video generation. Maximum length is 1800 characters.",
    )
    duration: nodetool.nodes.kie.video.RunwayGen3AlphaImageToVideo.Duration = Field(
        default=nodetool.nodes.kie.video.RunwayGen3AlphaImageToVideo.Duration.D5,
        description="Video duration in seconds. If 10-second video is selected, 1080p resolution cannot be used.",
    )
    quality: nodetool.nodes.kie.video.RunwayGen3AlphaImageToVideo.Quality = Field(
        default=nodetool.nodes.kie.video.RunwayGen3AlphaImageToVideo.Quality.R720P,
        description="Video resolution. If 1080p is selected, 10-second video cannot be generated.",
    )
    water_mark: str | OutputHandle[str] = connect_field(
        default="",
        description="Video watermark text content. An empty string indicates no watermark.",
    )
    call_back_url: str | OutputHandle[str] = connect_field(
        default="",
        description="Optional callback URL to receive task completion updates.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.RunwayGen3AlphaImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class RunwayGen3AlphaTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text using Runway's Gen-3 Alpha model via Kie.ai.

        kie, runway, gen-3, gen3alpha, video generation, ai, text-to-video

        Runway Gen-3 Alpha produces high-quality videos from text descriptions
        with advanced motion and temporal consistency.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.RunwayGen3AlphaTextToVideo.AspectRatio
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.RunwayGen3AlphaTextToVideo.Duration
    )
    Quality: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.RunwayGen3AlphaTextToVideo.Quality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )
    aspect_ratio: nodetool.nodes.kie.video.RunwayGen3AlphaTextToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.kie.video.RunwayGen3AlphaTextToVideo.AspectRatio.V16_9,
            description="The aspect ratio of the generated video. Required for text-to-video generation.",
        )
    )
    duration: nodetool.nodes.kie.video.RunwayGen3AlphaTextToVideo.Duration = Field(
        default=nodetool.nodes.kie.video.RunwayGen3AlphaTextToVideo.Duration.D5,
        description="Video duration in seconds. If 10-second video is selected, 1080p resolution cannot be used.",
    )
    quality: nodetool.nodes.kie.video.RunwayGen3AlphaTextToVideo.Quality = Field(
        default=nodetool.nodes.kie.video.RunwayGen3AlphaTextToVideo.Quality.R720P,
        description="Video resolution. If 1080p is selected, 10-second video cannot be generated.",
    )
    water_mark: str | OutputHandle[str] = connect_field(
        default="",
        description="Video watermark text content. An empty string indicates no watermark.",
    )
    call_back_url: str | OutputHandle[str] = connect_field(
        default="",
        description="Optional callback URL to receive task completion updates.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.RunwayGen3AlphaTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class SeedanceBaseNode(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Base class for Seedance (Bytedance) video generation nodes.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    )
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9,
        description="The aspect ratio of the generated video.",
    )
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P,
        description="The resolution of the video.",
    )
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5,
        description="The duration of the video in seconds.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceBaseNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class SeedanceV1LiteImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Bytedance 1.0 - image-to-video-lite via Kie.ai.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    )
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9,
        description="The aspect ratio of the generated video.",
    )
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P,
        description="The resolution of the video.",
    )
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5,
        description="The duration of the video in seconds.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text guide for the video generation.",
    )
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="First source image for the video generation.",
    )
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Second source image (optional).",
    )
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Third source image (optional).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceV1LiteImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class SeedanceV1LiteTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Bytedance 1.0 - text-to-video-lite via Kie.ai.

        kie, seedance, bytedance, video generation, ai, text-to-video, lite

        Seedance V1 Lite offers efficient text-to-video generation
        with good quality and faster processing times.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    )
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9,
        description="The aspect ratio of the generated video.",
    )
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P,
        description="The resolution of the video.",
    )
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5,
        description="The duration of the video in seconds.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceV1LiteTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class SeedanceV1ProFastImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Bytedance 1.0 - fast-image-to-video-pro via Kie.ai.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    )
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9,
        description="The aspect ratio of the generated video.",
    )
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P,
        description="The resolution of the video.",
    )
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5,
        description="The duration of the video in seconds.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="First source image for the video generation.",
    )
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Second source image (optional).",
    )
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Third source image (optional).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceV1ProFastImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class SeedanceV1ProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Bytedance 1.0 - image-to-video-pro via Kie.ai.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    )
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9,
        description="The aspect ratio of the generated video.",
    )
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P,
        description="The resolution of the video.",
    )
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5,
        description="The duration of the video in seconds.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text guide for the video generation.",
    )
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="First source image for the video generation.",
    )
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Second source image (optional).",
    )
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Third source image (optional).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceV1ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class SeedanceV1ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Bytedance 1.0 - text-to-video-pro via Kie.ai.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    )
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9,
        description="The aspect ratio of the generated video.",
    )
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P,
        description="The resolution of the video.",
    )
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(
        default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5,
        description="The duration of the video in seconds.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceV1ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Sora2BaseNode(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Base class for Sora 2 nodes via Kie.ai.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio
    )
    Sora2Frames: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2Frames

    aspect_ratio: nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio.LANDSCAPE,
        description="The aspect ratio of the generated video.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )
    n_frames: nodetool.nodes.kie.video.Sora2Frames = Field(
        default=nodetool.nodes.kie.video.Sora2Frames._10s,
        description="Number of frames for the video output.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Sora2BaseNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Sora2ProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from images using Sora 2 Pro via Kie.ai.

        kie, sora, openai, video generation, ai, image-to-video, pro

        Sora 2 Pro transforms images into high-quality videos with
        realistic motion and temporal consistency.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio
    )
    Sora2Frames: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2Frames

    aspect_ratio: nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio.LANDSCAPE,
        description="The aspect ratio of the generated video.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )
    n_frames: nodetool.nodes.kie.video.Sora2Frames = Field(
        default=nodetool.nodes.kie.video.Sora2Frames._10s,
        description="Number of frames for the video output.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text guide for the video generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The source image to animate.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Sora2ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Sora2ProStoryboard(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from storyboards using Sora 2 Pro via Kie.ai.

        kie, sora, openai, video generation, ai, storyboard, pro

        Sora 2 Pro creates videos from storyboard sequences with
        consistent characters and scenes across frames.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio
    )
    Sora2Frames: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2Frames

    aspect_ratio: nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio.LANDSCAPE,
        description="The aspect ratio of the generated video.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )
    n_frames: nodetool.nodes.kie.video.Sora2Frames = Field(
        default=nodetool.nodes.kie.video.Sora2Frames._10s,
        description="Number of frames for the video output.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="First source image for the video generation.",
    )
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Second source image (optional).",
    )
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Third source image (optional).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Sora2ProStoryboard

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Sora2ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text using Sora 2 Pro via Kie.ai.

        kie, sora, openai, video generation, ai, text-to-video, pro

        Sora 2 Pro generates high-quality videos from text descriptions
        with advanced motion and temporal consistency.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio
    )
    Sora2Frames: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2Frames

    aspect_ratio: nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio.LANDSCAPE,
        description="The aspect ratio of the generated video.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )
    n_frames: nodetool.nodes.kie.video.Sora2Frames = Field(
        default=nodetool.nodes.kie.video.Sora2Frames._10s,
        description="Number of frames for the video output.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Sora2ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Sora2TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text using Sora 2 Standard via Kie.ai.

        kie, sora, openai, video generation, ai, text-to-video, standard

        Sora 2 Standard generates quality videos from text descriptions
        with efficient processing and good visual quality.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio
    )
    Sora2Frames: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2Frames

    aspect_ratio: nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio.LANDSCAPE,
        description="The aspect ratio of the generated video.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )
    n_frames: nodetool.nodes.kie.video.Sora2Frames = Field(
        default=nodetool.nodes.kie.video.Sora2Frames._10s,
        description="Number of frames for the video output.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Sora2TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class TopazVideoUpscale(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Upscale and enhance videos using Topaz Labs AI via Kie.ai.
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.TopazVideoUpscale.Resolution
    )

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The video to upscale.",
    )
    resolution: nodetool.nodes.kie.video.TopazVideoUpscale.Resolution = Field(
        default=nodetool.nodes.kie.video.TopazVideoUpscale.Resolution.R1080P,
        description="Target resolution for upscaling.",
    )
    denoise: bool | OutputHandle[bool] = connect_field(
        default=True, description="Apply denoising to reduce artifacts."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.TopazVideoUpscale

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Veo31BaseNode(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Base class for Google Veo 3.1 video generation nodes via Kie.ai.

        kie, google, veo, veo3, veo3.1, video generation, ai, text-to-video, image-to-video

        Veo 3.1 offers native 9:16 vertical video support, multilingual prompt processing,
        and significant cost savings (25% of Google's direct API pricing).
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.Model
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio
    )

    model: nodetool.nodes.kie.video.Veo31BaseNode.Model = Field(
        default=nodetool.nodes.kie.video.Veo31BaseNode.Model.VEO3_FAST,
        description="The model to use for video generation.",
    )
    aspect_ratio: nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio.RATIO_16_9,
        description="Video aspect ratio.",
    )
    call_back_url: str | OutputHandle[str] = connect_field(
        default="", description="Optional callback URL for task completion."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Veo31BaseNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Veo31ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from images using Google's Veo 3.1 model via Kie.ai.

        kie, google, veo, veo3, veo3.1, video generation, ai, image-to-video, i2v

        Supports single image (image comes alive) or two images (first and last frames transition).
        For two images, the first image serves as the video's first frame and the second as the last frame.
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.Model
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio
    )

    model: nodetool.nodes.kie.video.Veo31BaseNode.Model = Field(
        default=nodetool.nodes.kie.video.Veo31BaseNode.Model.VEO3_FAST,
        description="The model to use for video generation.",
    )
    aspect_ratio: nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio.RATIO_16_9,
        description="Video aspect ratio.",
    )
    call_back_url: str | OutputHandle[str] = connect_field(
        default="", description="Optional callback URL for task completion."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Optional text prompt describing how the image should come alive.",
    )
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="First source image. Required. Serves as the video's first frame.",
    )
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Second source image (optional). If provided, serves as the video's last frame.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Veo31ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Veo31ReferenceToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from reference images using Google's Veo 3.1 Fast model via Kie.ai.

        kie, google, veo, veo3, veo3.1, video generation, ai, reference-to-video, material-to-video

        Material-to-video generation based on reference images. Only supports veo3_fast model
        and requires 1-3 reference images.
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.Model
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio
    )

    model: nodetool.nodes.kie.video.Veo31BaseNode.Model = Field(
        default=nodetool.nodes.kie.video.Veo31BaseNode.Model.VEO3_FAST,
        description="The model to use for video generation.",
    )
    aspect_ratio: nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio.RATIO_16_9,
        description="Video aspect ratio.",
    )
    call_back_url: str | OutputHandle[str] = connect_field(
        default="", description="Optional callback URL for task completion."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="Text prompt describing the desired video content.",
    )
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="First reference image. Required. Minimum 1, maximum 3 images.",
    )
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Second reference image (optional).",
    )
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Third reference image (optional).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Veo31ReferenceToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Veo31TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text using Google's Veo 3.1 via Kie.ai.

        kie, google, veo, veo3, veo3.1, video generation, ai, text-to-video

        Veo 3.1 offers native 9:16 vertical video support, multilingual prompt processing,
        and significant cost savings (25% of Google's direct API pricing).
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.Model
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio
    )

    model: nodetool.nodes.kie.video.Veo31BaseNode.Model = Field(
        default=nodetool.nodes.kie.video.Veo31BaseNode.Model.VEO3_FAST,
        description="The model to use for video generation.",
    )
    aspect_ratio: nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio = Field(
        default=nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio.RATIO_16_9,
        description="Video aspect ratio.",
    )
    call_back_url: str | OutputHandle[str] = connect_field(
        default="", description="Optional callback URL for task completion."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Veo31TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Wan26ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from images using Alibaba's Wan 2.6 model via Kie.ai.
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Wan26ImageToVideo.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Wan26ImageToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="First source image for the video generation.",
    )
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Second source image (optional).",
    )
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Third source image (optional).",
    )
    duration: nodetool.nodes.kie.video.Wan26ImageToVideo.Duration = Field(
        default=nodetool.nodes.kie.video.Wan26ImageToVideo.Duration.D5,
        description="The duration of the video in seconds.",
    )
    resolution: nodetool.nodes.kie.video.Wan26ImageToVideo.Resolution = Field(
        default=nodetool.nodes.kie.video.Wan26ImageToVideo.Resolution.R1080P,
        description="The resolution of the video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Wan26ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Wan26TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text using Alibaba's Wan 2.6 model via Kie.ai.

        kie, wan, alibaba, video generation, ai, text-to-video, 2.6

        Wan 2.6 generates high-quality videos from text descriptions
        with advanced motion and visual fidelity.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan26TextToVideo.Duration
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Wan26TextToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )
    duration: nodetool.nodes.kie.video.Wan26TextToVideo.Duration = Field(
        default=nodetool.nodes.kie.video.Wan26TextToVideo.Duration.D5,
        description="The duration of the video in seconds.",
    )
    resolution: nodetool.nodes.kie.video.Wan26TextToVideo.Resolution = Field(
        default=nodetool.nodes.kie.video.Wan26TextToVideo.Resolution.R1080P,
        description="The resolution of the video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Wan26TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class Wan26VideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from videos using Alibaba's Wan 2.6 model via Kie.ai.

        kie, wan, alibaba, video generation, ai, video-to-video, 2.6

        Wan 2.6 transforms and enhances existing videos with AI-powered
        editing and style transfer capabilities.
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Wan26VideoToVideo.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.Wan26VideoToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the changes.",
    )
    video1: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="First source video for the video-to-video task.",
    )
    video2: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Second source video (optional).",
    )
    video3: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Third source video (optional).",
    )
    duration: nodetool.nodes.kie.video.Wan26VideoToVideo.Duration = Field(
        default=nodetool.nodes.kie.video.Wan26VideoToVideo.Duration.D5,
        description="The duration of the video in seconds.",
    )
    resolution: nodetool.nodes.kie.video.Wan26VideoToVideo.Resolution = Field(
        default=nodetool.nodes.kie.video.Wan26VideoToVideo.Resolution.R1080P,
        description="The resolution of the video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Wan26VideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode


class WanMultiShotTextToVideoPro(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text using Alibaba's Wan 2.1 model via Kie.ai.

        kie, wan, alibaba, video generation, ai, text-to-video, multi-shot, 2.1

        Wan 2.1 Multi-Shot generates complex videos with multiple shots
        and scene transitions from text descriptions.
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A cinematic video with smooth motion, natural lighting, and high detail.",
        description="The text prompt describing the video.",
    )
    aspect_ratio: nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.AspectRatio = (
        Field(
            default=nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.AspectRatio.V16_9,
            description="The aspect ratio of the generated video.",
        )
    )
    resolution: nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Resolution = Field(
        default=nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Resolution.R1080P,
        description="The resolution of the video.",
    )
    duration: nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Duration = Field(
        default=nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Duration.D5,
        description="The duration of the video in seconds.",
    )
    remove_watermark: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove the watermark from the video."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.WanMultiShotTextToVideoPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
