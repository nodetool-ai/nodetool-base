# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class GrokImagineImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from images using xAI's Grok Imagine model via Kie.ai.

        kie, grok, xai, video generation, ai, image-to-video, multimodal

        Grok Imagine transforms images into videos using xAI's
        multimodal generation capabilities.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.GrokImagineImageToVideo.Duration

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Optional text guide for the animation.')
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The source image to animate.')
    duration: nodetool.nodes.kie.video.GrokImagineImageToVideo.Duration = Field(default=nodetool.nodes.kie.video.GrokImagineImageToVideo.Duration.MEDIUM, description='The duration tier of the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.GrokImagineImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class GrokImagineTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from text using xAI's Grok Imagine model via Kie.ai.

        kie, grok, xai, video generation, ai, text-to-video, multimodal

        Grok Imagine generates videos from text prompts using xAI's
        multimodal generation capabilities.
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.GrokImagineTextToVideo.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.GrokImagineTextToVideo.Duration

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')
    resolution: nodetool.nodes.kie.video.GrokImagineTextToVideo.Resolution = Field(default=nodetool.nodes.kie.video.GrokImagineTextToVideo.Resolution.R1080P, description='The resolution of the video.')
    duration: nodetool.nodes.kie.video.GrokImagineTextToVideo.Duration = Field(default=nodetool.nodes.kie.video.GrokImagineTextToVideo.Duration.MEDIUM, description='The duration tier of the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.GrokImagineTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class HailuoImageToVideoPro(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from images using MiniMax's Hailuo 2.3 Pro model via Kie.ai.

        kie, hailuo, minimax, video generation, ai, image-to-video, pro

        Hailuo 2.3 Pro offers the highest quality image-to-video generation with
        realistic motion, detailed textures, and cinematic quality.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.HailuoImageToVideoPro.Duration
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.HailuoImageToVideoPro.Resolution

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The reference image to animate into a video.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Optional text to guide the video generation.')
    duration: nodetool.nodes.kie.video.HailuoImageToVideoPro.Duration = Field(default=nodetool.nodes.kie.video.HailuoImageToVideoPro.Duration.D6, description='The duration of the video in seconds. 10s is not supported for 1080p.')
    resolution: nodetool.nodes.kie.video.HailuoImageToVideoPro.Resolution = Field(default=nodetool.nodes.kie.video.HailuoImageToVideoPro.Resolution.R768P, description='Video resolution.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.HailuoImageToVideoPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class HailuoImageToVideoStandard(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from images using MiniMax's Hailuo 2.3 Standard model via Kie.ai.

        kie, hailuo, minimax, video generation, ai, image-to-video, standard, fast

        Hailuo 2.3 Standard offers efficient image-to-video generation with good quality
        and faster processing times for practical use cases.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.HailuoImageToVideoStandard.Duration
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.HailuoImageToVideoStandard.Resolution

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The reference image to animate into a video.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Optional text to guide the video generation.')
    duration: nodetool.nodes.kie.video.HailuoImageToVideoStandard.Duration = Field(default=nodetool.nodes.kie.video.HailuoImageToVideoStandard.Duration.D6, description='The duration of the video in seconds. 10s is not supported for 1080p.')
    resolution: nodetool.nodes.kie.video.HailuoImageToVideoStandard.Resolution = Field(default=nodetool.nodes.kie.video.HailuoImageToVideoStandard.Resolution.R768P, description='Video resolution.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.HailuoImageToVideoStandard

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class HailuoTextToVideoPro(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from text using MiniMax's Hailuo 2.3 Pro model via Kie.ai.

        kie, hailuo, minimax, video generation, ai, text-to-video, pro

        Hailuo 2.3 Pro offers the highest quality text-to-video generation with
        realistic motion, detailed textures, and cinematic quality.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.HailuoTextToVideoPro.Duration
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.HailuoTextToVideoPro.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')
    duration: nodetool.nodes.kie.video.HailuoTextToVideoPro.Duration = Field(default=nodetool.nodes.kie.video.HailuoTextToVideoPro.Duration.D6, description='The duration of the video in seconds. 10s is not supported for 1080p.')
    resolution: nodetool.nodes.kie.video.HailuoTextToVideoPro.Resolution = Field(default=nodetool.nodes.kie.video.HailuoTextToVideoPro.Resolution.R768P, description='Video resolution.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.HailuoTextToVideoPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class HailuoTextToVideoStandard(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from text using MiniMax's Hailuo 2.3 Standard model via Kie.ai.

        kie, hailuo, minimax, video generation, ai, text-to-video, standard, fast
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.HailuoTextToVideoStandard.Duration
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.HailuoTextToVideoStandard.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')
    duration: nodetool.nodes.kie.video.HailuoTextToVideoStandard.Duration = Field(default=nodetool.nodes.kie.video.HailuoTextToVideoStandard.Duration.D6, description='The duration of the video in seconds. 10s is not supported for 1080p.')
    resolution: nodetool.nodes.kie.video.HailuoTextToVideoStandard.Resolution = Field(default=nodetool.nodes.kie.video.HailuoTextToVideoStandard.Resolution.R768P, description='Video resolution.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.HailuoTextToVideoStandard

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class InfinitalkV1(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos using Infinitalk v1 (image-to-video) via Kie.ai.
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.InfinitalkV1.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Optional text guide for the video generation.')
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The source image.')
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='The source audio track.')
    resolution: nodetool.nodes.kie.video.InfinitalkV1.Resolution = Field(default=nodetool.nodes.kie.video.InfinitalkV1.Resolution.R480P, description='Video resolution.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.InfinitalkV1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Kling25TurboImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from images using Kuaishou's Kling 2.5 Turbo model via Kie.ai.

        kie, kling, kuaishou, video generation, ai, image-to-video, turbo

        Transforms a static image into a dynamic video while preserving
        visual style, colors, lighting, and texture.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.Kling25TurboImageToVideo.Duration

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Text description to guide the video generation.')
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The source image to animate.')
    tail_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Tail frame image for the video (optional).')
    duration: nodetool.nodes.kie.video.Kling25TurboImageToVideo.Duration = Field(default=nodetool.nodes.kie.video.Kling25TurboImageToVideo.Duration.D5, description='Video duration in seconds.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Elements to avoid in the video.')
    cfg_scale: float | OutputHandle[float] = connect_field(default=0.5, description='The CFG scale for prompt adherence. Lower values allow more creativity.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Kling25TurboImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Kling25TurboTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from text using Kuaishou's Kling 2.5 Turbo model via Kie.ai.

        kie, kling, kuaishou, video generation, ai, text-to-video, turbo

        Kling 2.5 Turbo offers improved prompt adherence, fluid motion,
        consistent artistic styles, and realistic physics simulation.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.Kling25TurboTextToVideo.Duration
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.Kling25TurboTextToVideo.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')
    duration: nodetool.nodes.kie.video.Kling25TurboTextToVideo.Duration = Field(default=nodetool.nodes.kie.video.Kling25TurboTextToVideo.Duration.D5, description='Video duration in seconds.')
    aspect_ratio: nodetool.nodes.kie.video.Kling25TurboTextToVideo.AspectRatio = Field(default=nodetool.nodes.kie.video.Kling25TurboTextToVideo.AspectRatio.V16_9, description='The aspect ratio of the generated video.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Things to avoid in the generated video.')
    cfg_scale: float | OutputHandle[float] = connect_field(default=0.5, description='The CFG scale for prompt adherence. Lower values allow more creativity.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Kling25TurboTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class KlingAIAvatarPro(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate talking avatar videos using Kuaishou's Kling AI via Kie.ai.

        kie, kling, kuaishou, avatar, video generation, ai, talking-head, lip-sync

        Transforms a photo plus audio track into a lip-synced talking avatar video
        with natural-looking speech animation and consistent identity.
    """

    Mode: typing.ClassVar[type] = nodetool.nodes.kie.video.KlingAIAvatarPro.Mode

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The face/character image to animate.')
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='The audio track for lip-syncing.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Optional text to guide emotions and expressions.')
    mode: nodetool.nodes.kie.video.KlingAIAvatarPro.Mode = Field(default=nodetool.nodes.kie.video.KlingAIAvatarPro.Mode.STANDARD, description="Generation mode: 'standard' or 'pro' for higher quality.")

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.KlingAIAvatarPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class KlingAIAvatarStandard(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate talking avatar videos using Kuaishou's Kling AI via Kie.ai.

        kie, kling, kuaishou, avatar, video generation, ai, talking-head, lip-sync

        Transforms a photo plus audio track into a lip-synced talking avatar video
        with natural-looking speech animation and consistent identity.
    """

    Mode: typing.ClassVar[type] = nodetool.nodes.kie.video.KlingAIAvatarStandard.Mode

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The face/character image to animate.')
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='The audio track for lip-syncing.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Optional text to guide emotions and expressions.')
    mode: nodetool.nodes.kie.video.KlingAIAvatarStandard.Mode = Field(default=nodetool.nodes.kie.video.KlingAIAvatarStandard.Mode.STANDARD, description="Generation mode: 'standard' or 'pro' for higher quality.")

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.KlingAIAvatarStandard

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class KlingImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from images using Kuaishou's Kling 2.6 model via Kie.ai.

        kie, kling, kuaishou, video generation, ai, image-to-video, 2.6

        Transforms static images into dynamic videos with realistic motion
        and temporal consistency while preserving the original visual style.
    """

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Optional text prompt to guide the video generation.')
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='First source image for the video generation.')
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Second source image (optional).')
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Third source image (optional).')
    sound: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to generate sound for the video.')
    duration: int | OutputHandle[int] = connect_field(default=5, description='Video duration in seconds.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.KlingImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class KlingTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from text using Kuaishou's Kling 2.6 model via Kie.ai.

        kie, kling, kuaishou, video generation, ai, text-to-video, 2.6

        Kling 2.6 produces high-quality videos from text descriptions with
        realistic motion, natural lighting, and cinematic detail.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.KlingTextToVideo.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.KlingTextToVideo.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')
    aspect_ratio: nodetool.nodes.kie.video.KlingTextToVideo.AspectRatio = Field(default=nodetool.nodes.kie.video.KlingTextToVideo.AspectRatio.V16_9, description='The aspect ratio of the generated video.')
    duration: int | OutputHandle[int] = connect_field(default=5, description='Video duration in seconds.')
    resolution: nodetool.nodes.kie.video.KlingTextToVideo.Resolution = Field(default=nodetool.nodes.kie.video.KlingTextToVideo.Resolution.R768P, description='Video resolution.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducible results. Use -1 for random seed.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.KlingTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class SeedanceBaseNode(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Base class for Seedance (Bytedance) video generation nodes.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9, description='The aspect ratio of the generated video.')
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P, description='The resolution of the video.')
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5, description='The duration of the video in seconds.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceBaseNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class SeedanceV1LiteImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Bytedance 1.0 - image-to-video-lite via Kie.ai.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9, description='The aspect ratio of the generated video.')
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P, description='The resolution of the video.')
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5, description='The duration of the video in seconds.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Optional text guide for the video generation.')
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='First source image for the video generation.')
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Second source image (optional).')
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Third source image (optional).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceV1LiteImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class SeedanceV1LiteTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Bytedance 1.0 - text-to-video-lite via Kie.ai.

        kie, seedance, bytedance, video generation, ai, text-to-video, lite

        Seedance V1 Lite offers efficient text-to-video generation
        with good quality and faster processing times.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9, description='The aspect ratio of the generated video.')
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P, description='The resolution of the video.')
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5, description='The duration of the video in seconds.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceV1LiteTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class SeedanceV1ProFastImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Bytedance 1.0 - fast-image-to-video-pro via Kie.ai.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9, description='The aspect ratio of the generated video.')
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P, description='The resolution of the video.')
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5, description='The duration of the video in seconds.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='First source image for the video generation.')
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Second source image (optional).')
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Third source image (optional).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceV1ProFastImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class SeedanceV1ProImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Bytedance 1.0 - image-to-video-pro via Kie.ai.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9, description='The aspect ratio of the generated video.')
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P, description='The resolution of the video.')
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5, description='The duration of the video in seconds.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Optional text guide for the video generation.')
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='First source image for the video generation.')
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Second source image (optional).')
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Third source image (optional).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceV1ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class SeedanceV1ProTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Bytedance 1.0 - text-to-video-pro via Kie.ai.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.SeedanceBaseNode.Duration

    aspect_ratio: nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.AspectRatio.V16_9, description='The aspect ratio of the generated video.')
    resolution: nodetool.nodes.kie.video.SeedanceBaseNode.Resolution = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Resolution.R720P, description='The resolution of the video.')
    duration: nodetool.nodes.kie.video.SeedanceBaseNode.Duration = Field(default=nodetool.nodes.kie.video.SeedanceBaseNode.Duration.D5, description='The duration of the video in seconds.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.SeedanceV1ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Sora2BaseNode(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Base class for Sora 2 nodes via Kie.ai.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio
    Sora2Frames: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2Frames

    aspect_ratio: nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio.LANDSCAPE, description='The aspect ratio of the generated video.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')
    n_frames: nodetool.nodes.kie.video.Sora2Frames = Field(default=nodetool.nodes.kie.video.Sora2Frames._10s, description='Number of frames for the video output.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Sora2BaseNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Sora2ProImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from images using Sora 2 Pro via Kie.ai.

        kie, sora, openai, video generation, ai, image-to-video, pro

        Sora 2 Pro transforms images into high-quality videos with
        realistic motion and temporal consistency.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio
    Sora2Frames: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2Frames

    aspect_ratio: nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio.LANDSCAPE, description='The aspect ratio of the generated video.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')
    n_frames: nodetool.nodes.kie.video.Sora2Frames = Field(default=nodetool.nodes.kie.video.Sora2Frames._10s, description='Number of frames for the video output.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Optional text guide for the video generation.')
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The source image to animate.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Sora2ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Sora2ProStoryboard(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from storyboards using Sora 2 Pro via Kie.ai.

        kie, sora, openai, video generation, ai, storyboard, pro

        Sora 2 Pro creates videos from storyboard sequences with
        consistent characters and scenes across frames.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio
    Sora2Frames: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2Frames

    aspect_ratio: nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio.LANDSCAPE, description='The aspect ratio of the generated video.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')
    n_frames: nodetool.nodes.kie.video.Sora2Frames = Field(default=nodetool.nodes.kie.video.Sora2Frames._10s, description='Number of frames for the video output.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='First source image for the video generation.')
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Second source image (optional).')
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Third source image (optional).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Sora2ProStoryboard

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Sora2ProTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from text using Sora 2 Pro via Kie.ai.

        kie, sora, openai, video generation, ai, text-to-video, pro

        Sora 2 Pro generates high-quality videos from text descriptions
        with advanced motion and temporal consistency.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio
    Sora2Frames: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2Frames

    aspect_ratio: nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio.LANDSCAPE, description='The aspect ratio of the generated video.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')
    n_frames: nodetool.nodes.kie.video.Sora2Frames = Field(default=nodetool.nodes.kie.video.Sora2Frames._10s, description='Number of frames for the video output.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Sora2ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Sora2TextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from text using Sora 2 Standard via Kie.ai.

        kie, sora, openai, video generation, ai, text-to-video, standard

        Sora 2 Standard generates quality videos from text descriptions
        with efficient processing and good visual quality.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio
    Sora2Frames: typing.ClassVar[type] = nodetool.nodes.kie.video.Sora2Frames

    aspect_ratio: nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.Sora2BaseNode.AspectRatio.LANDSCAPE, description='The aspect ratio of the generated video.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')
    n_frames: nodetool.nodes.kie.video.Sora2Frames = Field(default=nodetool.nodes.kie.video.Sora2Frames._10s, description='Number of frames for the video output.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Sora2TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class TopazVideoUpscale(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Upscale and enhance videos using Topaz Labs AI via Kie.ai.
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.TopazVideoUpscale.Resolution

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='The video to upscale.')
    resolution: nodetool.nodes.kie.video.TopazVideoUpscale.Resolution = Field(default=nodetool.nodes.kie.video.TopazVideoUpscale.Resolution.R1080P, description='Target resolution for upscaling.')
    denoise: bool | OutputHandle[bool] = connect_field(default=True, description='Apply denoising to reduce artifacts.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.TopazVideoUpscale

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Veo31BaseNode(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Base class for Google Veo 3.1 video generation nodes via Kie.ai.

        kie, google, veo, veo3, veo3.1, video generation, ai, text-to-video, image-to-video

        Veo 3.1 offers native 9:16 vertical video support, multilingual prompt processing,
        and significant cost savings (25% of Google's direct API pricing).
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.Model
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio

    model: nodetool.nodes.kie.video.Veo31BaseNode.Model = Field(default=nodetool.nodes.kie.video.Veo31BaseNode.Model.VEO3_FAST, description='The model to use for video generation.')
    aspect_ratio: nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio.RATIO_16_9, description='Video aspect ratio.')
    call_back_url: str | OutputHandle[str] = connect_field(default='', description='Optional callback URL for task completion.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Veo31BaseNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Veo31ImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from images using Google's Veo 3.1 model via Kie.ai.

        kie, google, veo, veo3, veo3.1, video generation, ai, image-to-video, i2v

        Supports single image (image comes alive) or two images (first and last frames transition).
        For two images, the first image serves as the video's first frame and the second as the last frame.
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.Model
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio

    model: nodetool.nodes.kie.video.Veo31BaseNode.Model = Field(default=nodetool.nodes.kie.video.Veo31BaseNode.Model.VEO3_FAST, description='The model to use for video generation.')
    aspect_ratio: nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio.RATIO_16_9, description='Video aspect ratio.')
    call_back_url: str | OutputHandle[str] = connect_field(default='', description='Optional callback URL for task completion.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Optional text prompt describing how the image should come alive.')
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description="First source image. Required. Serves as the video's first frame.")
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description="Second source image (optional). If provided, serves as the video's last frame.")

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Veo31ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Veo31ReferenceToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from reference images using Google's Veo 3.1 Fast model via Kie.ai.

        kie, google, veo, veo3, veo3.1, video generation, ai, reference-to-video, material-to-video

        Material-to-video generation based on reference images. Only supports veo3_fast model
        and requires 1-3 reference images.
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.Model
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio

    model: nodetool.nodes.kie.video.Veo31BaseNode.Model = Field(default=nodetool.nodes.kie.video.Veo31BaseNode.Model.VEO3_FAST, description='The model to use for video generation.')
    aspect_ratio: nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio.RATIO_16_9, description='Video aspect ratio.')
    call_back_url: str | OutputHandle[str] = connect_field(default='', description='Optional callback URL for task completion.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='Text prompt describing the desired video content.')
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='First reference image. Required. Minimum 1, maximum 3 images.')
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Second reference image (optional).')
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Third reference image (optional).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Veo31ReferenceToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Veo31TextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from text using Google's Veo 3.1 model via Kie.ai.

        kie, google, veo, veo3, veo3.1, video generation, ai, text-to-video, t2v
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.Model
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio

    model: nodetool.nodes.kie.video.Veo31BaseNode.Model = Field(default=nodetool.nodes.kie.video.Veo31BaseNode.Model.VEO3_FAST, description='The model to use for video generation.')
    aspect_ratio: nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio = Field(default=nodetool.nodes.kie.video.Veo31BaseNode.AspectRatio.RATIO_16_9, description='Video aspect ratio.')
    call_back_url: str | OutputHandle[str] = connect_field(default='', description='Optional callback URL for task completion.')
    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the desired video content.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Veo31TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Wan25ImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from images using Alibaba's Wan 2.5 model via Kie.ai.

        kie, wan, alibaba, video generation, ai, image-to-video, 2.5

        Wan 2.5 transforms static images into dynamic videos with
        realistic motion and temporal consistency.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan25ImageToVideo.Duration
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan25ImageToVideo.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video motion.')
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The source image to animate into a video.')
    duration: nodetool.nodes.kie.video.Wan25ImageToVideo.Duration = Field(default=nodetool.nodes.kie.video.Wan25ImageToVideo.Duration.D5, description='The duration of the video in seconds.')
    resolution: nodetool.nodes.kie.video.Wan25ImageToVideo.Resolution = Field(default=nodetool.nodes.kie.video.Wan25ImageToVideo.Resolution.R1080P, description='Video resolution.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Things to avoid in the generated video.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt rewriting using LLM.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility. -1 for random.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Wan25ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Wan25TextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from text using Alibaba's Wan 2.5 model via Kie.ai.

        kie, wan, alibaba, video generation, ai, text-to-video, 2.5

        Wan 2.5 generates high-quality videos from text descriptions
        with advanced motion and visual fidelity.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan25TextToVideo.Duration
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan25TextToVideo.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan25TextToVideo.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')
    duration: nodetool.nodes.kie.video.Wan25TextToVideo.Duration = Field(default=nodetool.nodes.kie.video.Wan25TextToVideo.Duration.D5, description='The duration of the video in seconds.')
    aspect_ratio: nodetool.nodes.kie.video.Wan25TextToVideo.AspectRatio = Field(default=nodetool.nodes.kie.video.Wan25TextToVideo.AspectRatio.V16_9, description='The aspect ratio of the generated video.')
    resolution: nodetool.nodes.kie.video.Wan25TextToVideo.Resolution = Field(default=nodetool.nodes.kie.video.Wan25TextToVideo.Resolution.R1080P, description='Video resolution.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Things to avoid in the generated video.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt rewriting using LLM.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility. -1 for random.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Wan25TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Wan26ImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from images using Alibaba's Wan 2.6 model via Kie.ai.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan26ImageToVideo.Duration
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan26ImageToVideo.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')
    image1: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='First source image for the video generation.')
    image2: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Second source image (optional).')
    image3: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Third source image (optional).')
    duration: nodetool.nodes.kie.video.Wan26ImageToVideo.Duration = Field(default=nodetool.nodes.kie.video.Wan26ImageToVideo.Duration.D5, description='The duration of the video in seconds.')
    resolution: nodetool.nodes.kie.video.Wan26ImageToVideo.Resolution = Field(default=nodetool.nodes.kie.video.Wan26ImageToVideo.Resolution.R1080P, description='The resolution of the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Wan26ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Wan26TextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from text using Alibaba's Wan 2.6 model via Kie.ai.

        kie, wan, alibaba, video generation, ai, text-to-video, 2.6

        Wan 2.6 generates high-quality videos from text descriptions
        with advanced motion and visual fidelity.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan26TextToVideo.Duration
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan26TextToVideo.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')
    duration: nodetool.nodes.kie.video.Wan26TextToVideo.Duration = Field(default=nodetool.nodes.kie.video.Wan26TextToVideo.Duration.D5, description='The duration of the video in seconds.')
    resolution: nodetool.nodes.kie.video.Wan26TextToVideo.Resolution = Field(default=nodetool.nodes.kie.video.Wan26TextToVideo.Resolution.R1080P, description='The resolution of the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Wan26TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class Wan26VideoToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from videos using Alibaba's Wan 2.6 model via Kie.ai.

        kie, wan, alibaba, video generation, ai, video-to-video, 2.6

        Wan 2.6 transforms and enhances existing videos with AI-powered
        editing and style transfer capabilities.
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan26VideoToVideo.Duration
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.Wan26VideoToVideo.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the changes.')
    video1: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='First source video for the video-to-video task.')
    video2: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='Second source video (optional).')
    video3: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='Third source video (optional).')
    duration: nodetool.nodes.kie.video.Wan26VideoToVideo.Duration = Field(default=nodetool.nodes.kie.video.Wan26VideoToVideo.Duration.D5, description='The duration of the video in seconds.')
    resolution: nodetool.nodes.kie.video.Wan26VideoToVideo.Resolution = Field(default=nodetool.nodes.kie.video.Wan26VideoToVideo.Resolution.R1080P, description='The resolution of the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.Wan26VideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.video
from nodetool.workflows.base_node import BaseNode

class WanMultiShotTextToVideoPro(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Generate videos from text using Alibaba's Wan 2.1 model via Kie.ai.

        kie, wan, alibaba, video generation, ai, text-to-video, multi-shot, 2.1

        Wan 2.1 Multi-Shot generates complex videos with multiple shots
        and scene transitions from text descriptions.
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Duration

    prompt: str | OutputHandle[str] = connect_field(default='A cinematic video with smooth motion, natural lighting, and high detail.', description='The text prompt describing the video.')
    aspect_ratio: nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.AspectRatio = Field(default=nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.AspectRatio.V16_9, description='The aspect ratio of the generated video.')
    resolution: nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Resolution = Field(default=nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Resolution.R1080P, description='The resolution of the video.')
    duration: nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Duration = Field(default=nodetool.nodes.kie.video.WanMultiShotTextToVideoPro.Duration.D5, description='The duration of the video in seconds.')
    remove_watermark: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to remove the watermark from the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.video.WanMultiShotTextToVideoPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


