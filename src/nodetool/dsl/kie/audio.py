# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class AddInstrumental(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Add instrumental accompaniment to uploaded audio via Suno AI.

        kie, suno, music, audio, ai, instrumental, accompaniment, upload

        Uploads a source track (e.g., vocals/stems) and generates a backing track.
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.audio.AddInstrumental.Model
    VocalGender: typing.ClassVar[type] = nodetool.nodes.kie.audio.VocalGender

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='Source audio to upload for instrumental generation.')
    title: str | OutputHandle[str] = connect_field(default='', description='Title of the generated music.')
    tags: str | OutputHandle[str] = connect_field(default='', description='Music styles or tags to include in the generated music.')
    negative_tags: str | OutputHandle[str] = connect_field(default='', description='Music styles or characteristics to exclude.')
    model: nodetool.nodes.kie.audio.AddInstrumental.Model = Field(default=nodetool.nodes.kie.audio.AddInstrumental.Model.V4_5PLUS, description='Suno model version to use.')
    vocal_gender: nodetool.nodes.kie.audio.VocalGender = Field(default=nodetool.nodes.kie.audio.VocalGender.UNSPECIFIED, description='Vocal gender preference.')
    style_weight: float | OutputHandle[float] = connect_field(default=0.0, description='Strength of adherence to style (0-1).')
    weirdness_constraint: float | OutputHandle[float] = connect_field(default=0.0, description='Creative deviation control (0-1).')
    audio_weight: float | OutputHandle[float] = connect_field(default=0.0, description='Balance weight for audio features (0-1).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.AddInstrumental

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class AddVocals(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Add AI vocals to uploaded audio via Suno AI.

        kie, suno, music, audio, ai, vocals, singing, upload

        Uploads an instrumental track and generates vocal layers on top.
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.audio.AddVocals.Model
    VocalGender: typing.ClassVar[type] = nodetool.nodes.kie.audio.VocalGender

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='Source audio to upload for vocal generation.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt describing lyric content and singing style.')
    title: str | OutputHandle[str] = connect_field(default='', description='Title of the generated music.')
    style: str | OutputHandle[str] = connect_field(default='', description='Music style for vocal generation.')
    tags: str | OutputHandle[str] = connect_field(default='', description='Optional music tags to include in the generation.')
    negative_tags: str | OutputHandle[str] = connect_field(default='', description='Excluded music styles or elements.')
    model: nodetool.nodes.kie.audio.AddVocals.Model = Field(default=nodetool.nodes.kie.audio.AddVocals.Model.V4_5PLUS, description='Suno model version to use.')
    vocal_gender: nodetool.nodes.kie.audio.VocalGender = Field(default=nodetool.nodes.kie.audio.VocalGender.UNSPECIFIED, description='Vocal gender preference.')
    style_weight: float | OutputHandle[float] = connect_field(default=0.0, description='Strength of adherence to style (0-1).')
    weirdness_constraint: float | OutputHandle[float] = connect_field(default=0.0, description='Creative deviation control (0-1).')
    audio_weight: float | OutputHandle[float] = connect_field(default=0.0, description='Balance weight for audio features (0-1).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.AddVocals

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class BoostMusicStyle(SingleOutputGraphNode[types.TextRef], GraphNode[types.TextRef]):
    """
    Boost music style text using Suno V4_5 style generation.

        kie, suno, music, style, prompt, enhancement
    """

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    content: str | OutputHandle[str] = connect_field(default='', description='Style description to enhance.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.BoostMusicStyle

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class ConvertToWav(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Convert a generated music track to WAV format.

        kie, suno, music, audio, wav, conversion
    """

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    task_id: str | OutputHandle[str] = connect_field(default='', description='Original music task ID.')
    audio_id: str | OutputHandle[str] = connect_field(default='', description='Audio ID to convert to WAV.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.ConvertToWav

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class CoverAudio(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Cover an uploaded audio track using Suno AI via Kie.ai.

        kie, suno, music, audio, ai, cover, upload, style transfer

        Uploads a source track and generates a covered version in a new style while
        retaining the original melody.
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.audio.CoverAudio.Model
    VocalGender: typing.ClassVar[type] = nodetool.nodes.kie.audio.VocalGender

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    custom_mode: bool | OutputHandle[bool] = connect_field(default=False, description='Enable custom mode for detailed control over style and title.')
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='Source audio to upload for covering.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='Music description or lyrics. In custom mode, this is used as lyrics when instrumental is false. In non-custom mode, this is the core idea.')
    style: str | OutputHandle[str] = connect_field(default='', description='Music style specification (required in custom mode).')
    title: str | OutputHandle[str] = connect_field(default='', description='Track title (required in custom mode).')
    instrumental: bool | OutputHandle[bool] = connect_field(default=False, description='Generate instrumental-only (no vocals).')
    model: nodetool.nodes.kie.audio.CoverAudio.Model = Field(default=nodetool.nodes.kie.audio.CoverAudio.Model.V4_5PLUS, description='Suno model version to use.')
    negative_tags: str | OutputHandle[str] = connect_field(default='', description='Music styles or traits to exclude from the generated audio.')
    vocal_gender: nodetool.nodes.kie.audio.VocalGender = Field(default=nodetool.nodes.kie.audio.VocalGender.UNSPECIFIED, description='Vocal gender preference (custom mode only).')
    style_weight: float | OutputHandle[float] = connect_field(default=0.0, description='Strength of adherence to style (0-1).')
    weirdness_constraint: float | OutputHandle[float] = connect_field(default=0.0, description='Creative deviation control (0-1).')
    audio_weight: float | OutputHandle[float] = connect_field(default=0.0, description='Balance weight for audio features (0-1).')
    persona_id: str | OutputHandle[str] = connect_field(default='', description='Persona ID to apply (custom mode only).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.CoverAudio

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class ElevenLabsAudioIsolation(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Isolate speech from audio using ElevenLabs AI via Kie.ai.

        kie, elevenlabs, audio-isolation, speech, noise-removal, ai

        ElevenLabs Audio Isolation uses AI to remove background noise, music,
        and interference while preserving clear, natural speech.

        Use cases:
        - Clean up podcast and interview recordings
        - Remove background noise from audio
        - Isolate speech for professional recordings
        - Prepare audio for transcription or production
    """

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='Audio file to process for speech isolation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.ElevenLabsAudioIsolation

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class ElevenLabsSoundEffect(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Generate sound effects using ElevenLabs AI via Kie.ai.

        kie, elevenlabs, sound-effect, sfx, audio, ai

        ElevenLabs Sound Effect V2 generates audio from text descriptions,
        supporting clips up to 20+ seconds with seamless looping and 48kHz audio.

        Use cases:
        - Generate custom sound effects for videos
        - Create ambient sounds for games and applications
        - Produce foley effects from text descriptions
        - Generate audio elements for creative projects
    """

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    text: str | OutputHandle[str] = connect_field(default='', description='Text description of the sound effect to generate.')
    duration_seconds: float | OutputHandle[float] = connect_field(default=5.0, description='Duration of the sound effect in seconds (up to 22 seconds).')
    prompt_influence: float | OutputHandle[float] = connect_field(default=0.3, description='How strongly the prompt influences generation (0-1).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.ElevenLabsSoundEffect

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class ElevenLabsSpeechToText(SingleOutputGraphNode[types.TextRef], GraphNode[types.TextRef]):
    """
    Transcribe speech to text using ElevenLabs AI via Kie.ai.

        kie, elevenlabs, speech-to-text, transcription, stt, ai

        ElevenLabs Speech to Text (Scribe v1) delivers state-of-the-art transcription
        with multilingual support, speaker diarization, and audio-event tagging.

        Use cases:
        - Transcribe podcasts and interviews
        - Create subtitles for videos
        - Convert audio recordings to text
        - Generate meeting transcripts with speaker labels
    """

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='Audio file to transcribe.')
    language_code: str | OutputHandle[str] = connect_field(default='', description="Language code (e.g., 'en', 'es', 'fr'). Leave empty for auto-detection.")
    diarization: bool | OutputHandle[bool] = connect_field(default=False, description='Enable speaker diarization to identify different speakers.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.ElevenLabsSpeechToText

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class ElevenLabsTextToSpeech(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Generate speech using ElevenLabs AI via Kie.ai.

        kie, elevenlabs, tts, text-to-speech, voice, audio, ai, speech synthesis

        Creates natural-sounding speech from text using ElevenLabs' voice models.
        Supports multiple voices, stability controls, and multilingual output.

        Use cases:
        - Generate voiceovers for videos and podcasts
        - Create audiobooks and narrated content
        - Produce natural-sounding speech for applications
        - Generate speech in multiple languages and voices
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.audio.ElevenLabsTextToSpeech.Model

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    text: str | OutputHandle[str] = connect_field(default='', description='The text to convert to speech.')
    voice: str | OutputHandle[str] = connect_field(default='Rachel', description='The voice ID to use for synthesis. Common voices: Rachel, Adam, Bella, Antoni.')
    stability: float | OutputHandle[float] = connect_field(default=0.5, description='Stability of the voice output. Lower values are more expressive, higher values are more consistent.')
    similarity_boost: float | OutputHandle[float] = connect_field(default=0.75, description='How closely to clone the voice characteristics. Higher values match the voice more closely.')
    style: float | OutputHandle[float] = connect_field(default=0.0, description='Style parameter for voice expression. Range 0.0 to 1.0.')
    speed: float | OutputHandle[float] = connect_field(default=1.0, description='Speed of the speech. Range 0.5 to 1.5.')
    language_code: str | OutputHandle[str] = connect_field(default='', description="Language code for multilingual TTS (e.g., 'en', 'es', 'fr', 'de'). Leave empty for auto-detection.")
    model: nodetool.nodes.kie.audio.ElevenLabsTextToSpeech.Model = Field(default=nodetool.nodes.kie.audio.ElevenLabsTextToSpeech.Model.TURBO_2_5, description='ElevenLabs model version to use.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.ElevenLabsTextToSpeech

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class ElevenLabsV3Dialogue(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Generate expressive dialogue using ElevenLabs V3 via Kie.ai.

        kie, elevenlabs, v3, dialogue, tts, text-to-speech, multi-speaker, ai

        ElevenLabs Eleven V3 enables expressive multilingual Text to Dialogue
        with audio tag control, multi-speaker support, and natural delivery.

        Use cases:
        - Generate dialogue for storytelling applications
        - Create multi-speaker audio content
        - Produce expressive voiceovers with audio tags
        - Generate natural conversation audio
    """

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    text: str | OutputHandle[str] = connect_field(default='', description='The dialogue text to convert to speech. Supports audio tags for control.')
    voice: str | OutputHandle[str] = connect_field(default='Rachel', description='Primary voice ID to use for synthesis.')
    stability: float | OutputHandle[float] = connect_field(default=0.5, description='Stability of the voice output (0-1).')
    similarity_boost: float | OutputHandle[float] = connect_field(default=0.75, description='Voice clone similarity (0-1).')
    style: float | OutputHandle[float] = connect_field(default=0.0, description='Style expression parameter (0-1).')
    speed: float | OutputHandle[float] = connect_field(default=1.0, description='Speech speed (0.5-1.5).')
    language_code: str | OutputHandle[str] = connect_field(default='', description='Language code for multilingual output. Leave empty for auto-detection.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.ElevenLabsV3Dialogue

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class ExtendMusic(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Extend music using Suno AI via Kie.ai.

        kie, suno, music, audio, ai, extension, continuation, remix

        Extends an existing track by continuing from a specified time point.
        Can reuse original parameters or override them with custom settings.
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.audio.ExtendMusic.Model
    VocalGender: typing.ClassVar[type] = nodetool.nodes.kie.audio.VocalGender

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    default_param_flag: bool | OutputHandle[bool] = connect_field(default=False, description='If true, use custom parameters (prompt/style/title/continue_at). If false, inherit parameters from the source audio.')
    audio_id: str | OutputHandle[str] = connect_field(default='', description='Audio ID to extend.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='Description of the desired extension content.')
    style: str | OutputHandle[str] = connect_field(default='', description='Music style for the extension (required for custom params).')
    title: str | OutputHandle[str] = connect_field(default='', description='Title for the extended track (required for custom params).')
    continue_at: float | OutputHandle[float] = connect_field(default=0.0, description='Time in seconds to start extending from (required for custom params).')
    model: nodetool.nodes.kie.audio.ExtendMusic.Model = Field(default=nodetool.nodes.kie.audio.ExtendMusic.Model.V4_5PLUS, description='Suno model version to use (must match source audio).')
    negative_tags: str | OutputHandle[str] = connect_field(default='', description='Music styles or traits to exclude from the extension.')
    vocal_gender: nodetool.nodes.kie.audio.VocalGender = Field(default=nodetool.nodes.kie.audio.VocalGender.UNSPECIFIED, description='Vocal gender preference.')
    style_weight: float | OutputHandle[float] = connect_field(default=0.0, description='Strength of adherence to style (0-1).')
    weirdness_constraint: float | OutputHandle[float] = connect_field(default=0.0, description='Creative deviation control (0-1).')
    audio_weight: float | OutputHandle[float] = connect_field(default=0.0, description='Balance weight for audio features (0-1).')
    persona_id: str | OutputHandle[str] = connect_field(default='', description='Persona ID to apply (custom params only).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.ExtendMusic

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class GenerateLyrics(SingleOutputGraphNode[list[dict[str, Any]]], GraphNode[list[dict[str, Any]]]):
    """
    Generate lyrics based on a text prompt via Kie.ai.

        kie, suno, lyrics, text, songwriting, prompt
    """

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt describing the theme, mood, or style of the lyrics.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.GenerateLyrics

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class GenerateMusic(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Generate music using Suno AI via Kie.ai.

        kie, suno, music, audio, ai, generation, vocals, instrumental

        Creates full tracks with vocals and instrumentals using Suno models.
        Supports custom mode for strict lyric control and non-custom mode for easy prompts.

        Use cases:
        - Generate background music for projects
        - Create AI-composed songs with vocals
        - Produce instrumentals for content
        - Generate music in various genres and styles
    """

    Model: typing.ClassVar[type] = nodetool.nodes.kie.audio.GenerateMusic.Model
    VocalGender: typing.ClassVar[type] = nodetool.nodes.kie.audio.VocalGender

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    custom_mode: bool | OutputHandle[bool] = connect_field(default=False, description='Enable custom mode for detailed control over style and title.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='Music description or lyrics. In custom mode, this is used as lyrics when instrumental is false. In non-custom mode, this is the core idea.')
    style: str | OutputHandle[str] = connect_field(default='', description='Music style specification (required in custom mode).')
    title: str | OutputHandle[str] = connect_field(default='', description='Track title (required in custom mode, max 80 characters).')
    instrumental: bool | OutputHandle[bool] = connect_field(default=False, description='Generate instrumental-only (no vocals).')
    model: nodetool.nodes.kie.audio.GenerateMusic.Model = Field(default=nodetool.nodes.kie.audio.GenerateMusic.Model.V4_5PLUS, description='Suno model version to use.')
    negative_tags: str | OutputHandle[str] = connect_field(default='', description='Music styles or traits to exclude from the generated audio.')
    vocal_gender: nodetool.nodes.kie.audio.VocalGender = Field(default=nodetool.nodes.kie.audio.VocalGender.UNSPECIFIED, description='Vocal gender preference (custom mode only).')
    style_weight: float | OutputHandle[float] = connect_field(default=0.0, description='Strength of adherence to style (0-1).')
    weirdness_constraint: float | OutputHandle[float] = connect_field(default=0.0, description='Creative deviation control (0-1).')
    audio_weight: float | OutputHandle[float] = connect_field(default=0.0, description='Balance weight for audio features (0-1).')
    persona_id: str | OutputHandle[str] = connect_field(default='', description='Persona ID to apply (custom mode only).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.GenerateMusic

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class GenerateMusicCover(SingleOutputGraphNode[list[types.ImageRef]], GraphNode[list[types.ImageRef]]):
    """
    Generate cover images for a Suno music task.

        kie, suno, music, cover, image, artwork
    """

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    task_id: str | OutputHandle[str] = connect_field(default='', description='Original music task ID.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.GenerateMusicCover

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class GenerateMusicVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Create a music video visualization for a generated track.

        kie, suno, music, video, mp4, visualization
    """

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    task_id: str | OutputHandle[str] = connect_field(default='', description='Original music task ID.')
    audio_id: str | OutputHandle[str] = connect_field(default='', description='Audio ID to visualize.')
    author: str | OutputHandle[str] = connect_field(default='', description='Optional artist/creator name (max 50 chars).')
    domain_name: str | OutputHandle[str] = connect_field(default='', description='Optional domain watermark (max 50 chars).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.GenerateMusicVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class GetTimestampedLyrics(SingleOutputGraphNode[types.TextRef], GraphNode[types.TextRef]):
    """
    Retrieve timestamped lyrics for a generated Suno track.

        kie, suno, music, audio, lyrics, timestamps, karaoke

        Fetches word-level alignment and waveform data for a specific task/audio pair.
    """

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    task_id: str | OutputHandle[str] = connect_field(default='', description='Task ID from Generate Music or Extend Music.')
    audio_id: str | OutputHandle[str] = connect_field(default='', description='Audio ID for the specific track.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.GetTimestampedLyrics

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class ReplaceMusicSection(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """
    Replace a section of a generated Suno track.

        kie, suno, music, replace, edit, infill

        Regenerates a time range and blends it into the original track.
    """

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    task_id: str | OutputHandle[str] = connect_field(default='', description='Original music task ID.')
    audio_id: str | OutputHandle[str] = connect_field(default='', description='Audio ID to replace.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt describing the replacement segment content.')
    tags: str | OutputHandle[str] = connect_field(default='', description='Music style tags.')
    title: str | OutputHandle[str] = connect_field(default='', description='Music title.')
    infill_start_s: float | OutputHandle[float] = connect_field(default=0.0, description='Start time point for replacement (seconds).')
    infill_end_s: float | OutputHandle[float] = connect_field(default=0.0, description='End time point for replacement (seconds).')
    negative_tags: str | OutputHandle[str] = connect_field(default='', description='Excluded music styles for the replacement segment.')
    full_lyrics: str | OutputHandle[str] = connect_field(default='', description='Full lyrics after modification.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.ReplaceMusicSection

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.kie.audio
from nodetool.workflows.base_node import BaseNode

class VocalStemSeparation(SingleOutputGraphNode[dict[str, types.AudioRef]], GraphNode[dict[str, types.AudioRef]]):
    """
    Separate a track into vocal/instrument stems via Suno.

        kie, suno, music, stems, separation, vocals, instrumental
    """

    SeparationType: typing.ClassVar[type] = nodetool.nodes.kie.audio.VocalStemSeparation.SeparationType

    timeout_seconds: int | OutputHandle[int] = connect_field(default=0, description='Timeout in seconds for API calls (0 = use default)')
    task_id: str | OutputHandle[str] = connect_field(default='', description='Original music task ID.')
    audio_id: str | OutputHandle[str] = connect_field(default='', description='Audio ID to separate.')
    separation_type: nodetool.nodes.kie.audio.VocalStemSeparation.SeparationType = Field(default=nodetool.nodes.kie.audio.VocalStemSeparation.SeparationType.SEPARATE_VOCAL, description='Separation mode.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.kie.audio.VocalStemSeparation

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


