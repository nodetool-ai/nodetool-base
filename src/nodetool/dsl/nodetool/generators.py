# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.generators
from nodetool.workflows.base_node import BaseNode


class ChartGenerator(
    SingleOutputGraphNode[types.PlotlyConfig], GraphNode[types.PlotlyConfig]
):
    """

    LLM Agent to create Plotly Express charts based on natural language descriptions.
    llm, data visualization, charts

    Use cases:
    - Generating interactive charts from natural language descriptions
    - Creating data visualizations with minimal configuration
    - Converting data analysis requirements into visual representations
    """

    model: types.LanguageModel | OutputHandle[types.LanguageModel] = connect_field(
        default=types.LanguageModel(
            type="language_model",
            provider=nodetool.metadata.types.Provider.Empty,
            id="",
            name="",
            path=None,
            supported_tasks=[],
        ),
        description="The model to use for chart generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Natural language description of the desired chart"
    )
    data: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            columns=None,
        ),
        description="The data to visualize",
    )
    max_tokens: int | OutputHandle[int] = connect_field(
        default=4096, description="The maximum number of tokens to generate."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.generators.ChartGenerator

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.generators
from nodetool.workflows.base_node import BaseNode


class DataGenerator(
    GraphNode[nodetool.nodes.nodetool.generators.DataGenerator.OutputType]
):
    """

    LLM Agent to create a dataframe based on a user prompt.
    llm, dataframe creation, data structuring

    Use cases:
    - Generating structured data from natural language descriptions
    - Creating sample datasets for testing or demonstration
    - Converting unstructured text into tabular format
    """

    model: types.LanguageModel | OutputHandle[types.LanguageModel] = connect_field(
        default=types.LanguageModel(
            type="language_model",
            provider=nodetool.metadata.types.Provider.Empty,
            id="",
            name="",
            path=None,
            supported_tasks=[],
        ),
        description="The model to use for data generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The user prompt"
    )
    input_text: str | OutputHandle[str] = connect_field(
        default="", description="The input text to be analyzed by the agent."
    )
    max_tokens: int | OutputHandle[int] = connect_field(
        default=4096, description="The maximum number of tokens to generate."
    )
    columns: types.RecordType | OutputHandle[types.RecordType] = connect_field(
        default=types.RecordType(type="record_type", columns=[]),
        description="The columns to use in the dataframe.",
    )

    @property
    def out(self) -> "DataGeneratorOutputs":
        return DataGeneratorOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.generators.DataGenerator

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class DataGeneratorOutputs(OutputsProxy):
    @property
    def record(self) -> OutputHandle[dict]:
        return typing.cast(OutputHandle[dict], self["record"])

    @property
    def dataframe(self) -> OutputHandle[nodetool.metadata.types.DataframeRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.DataframeRef], self["dataframe"]
        )

    @property
    def index(self) -> OutputHandle[int]:
        return typing.cast(OutputHandle[int], self["index"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.generators
from nodetool.workflows.base_node import BaseNode


class ListGenerator(
    GraphNode[nodetool.nodes.nodetool.generators.ListGenerator.OutputType]
):
    """

    LLM Agent to create a stream of strings based on a user prompt.
    llm, text streaming

    Use cases:
    - Generating text from natural language descriptions
    - Streaming responses from an LLM
    """

    model: types.LanguageModel | OutputHandle[types.LanguageModel] = connect_field(
        default=types.LanguageModel(
            type="language_model",
            provider=nodetool.metadata.types.Provider.Empty,
            id="",
            name="",
            path=None,
            supported_tasks=[],
        ),
        description="The model to use for string generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The user prompt"
    )
    input_text: str | OutputHandle[str] = connect_field(
        default="", description="The input text to be analyzed by the agent."
    )
    max_tokens: int | OutputHandle[int] = connect_field(
        default=4096, description="The maximum number of tokens to generate."
    )

    @property
    def out(self) -> "ListGeneratorOutputs":
        return ListGeneratorOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.generators.ListGenerator

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class ListGeneratorOutputs(OutputsProxy):
    @property
    def item(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["item"])

    @property
    def index(self) -> OutputHandle[int]:
        return typing.cast(OutputHandle[int], self["index"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.generators
from nodetool.workflows.base_node import BaseNode


class SVGGenerator(
    SingleOutputGraphNode[list[types.SVGElement]], GraphNode[list[types.SVGElement]]
):
    """

    LLM Agent to create SVG elements based on user prompts.
    svg, generator, vector, graphics

    Use cases:
    - Creating vector graphics from text descriptions
    - Generating scalable illustrations
    - Creating custom icons and diagrams
    """

    model: types.LanguageModel | OutputHandle[types.LanguageModel] = connect_field(
        default=types.LanguageModel(
            type="language_model",
            provider=nodetool.metadata.types.Provider.Empty,
            id="",
            name="",
            path=None,
            supported_tasks=[],
        ),
        description="The language model to use for SVG generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The user prompt for SVG generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use for generation",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Audio to use for generation",
    )
    max_tokens: int | OutputHandle[int] = connect_field(
        default=8192, description="The maximum number of tokens to generate."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.generators.SVGGenerator

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import (
    OutputHandle,
    OutputsProxy,
    DynamicOutputsProxy,
    connect_field,
)
import nodetool.nodes.nodetool.generators
from nodetool.workflows.base_node import BaseNode


class StructuredOutputGenerator(GraphNode[dict[str, Any]]):
    """

    Generate structured JSON objects from instructions using LLM providers.
    data-generation, structured-data, json, synthesis

    Specialized for creating structured information:
    - Generating JSON that follows dynamic schemas
    - Fabricating records from requirements and guidance
    - Simulating sample data for downstream workflows
    - Producing consistent structured outputs for testing
    """

    system_prompt: str | OutputHandle[str] = connect_field(
        default="\nYou are a structured data generator focused on JSON outputs.\n\nGoal\n- Produce a high-quality JSON object that matches <JSON_SCHEMA> using the guidance in <INSTRUCTIONS> and any supplemental <CONTEXT>.\n\nOutput format (MANDATORY)\n- Output exactly ONE fenced code block labeled json containing ONLY the JSON object:\n\n  ```json\n  { ...single JSON object matching <JSON_SCHEMA>... }\n  ```\n\n- No additional prose before or after the block.\n\nGeneration rules\n- Invent plausible, internally consistent values when not explicitly provided.\n- Honor all constraints from <JSON_SCHEMA> (types, enums, ranges, formats).\n- Prefer ISO 8601 for dates/times when applicable.\n- Ensure numbers respect reasonable magnitudes and relationships described in <INSTRUCTIONS>.\n- Avoid referencing external sources; rely solely on the provided guidance.\n\nValidation\n- Ensure the final JSON validates against <JSON_SCHEMA> exactly.\n",
        description="The system prompt guiding JSON generation.",
    )
    model: types.LanguageModel | OutputHandle[types.LanguageModel] = connect_field(
        default=types.LanguageModel(
            type="language_model",
            provider=nodetool.metadata.types.Provider.Empty,
            id="",
            name="",
            path=None,
            supported_tasks=[],
        ),
        description="Model to use for structured generation.",
    )
    instructions: str | OutputHandle[str] = connect_field(
        default="", description="Detailed instructions for the structured output."
    )
    context: str | OutputHandle[str] = connect_field(
        default="", description="Optional context to ground the generation."
    )
    max_tokens: int | OutputHandle[int] = connect_field(
        default=4096, description="The maximum number of tokens to generate."
    )
    context_window: int | OutputHandle[int] = connect_field(
        default=4096, description=None
    )

    def __init__(
        self,
        *,
        dynamic_outputs: dict[str, typing.Any] | None = None,
        **kwargs: typing.Any,
    ) -> None:
        """
        Initialize a StructuredOutputGenerator node.

        Dynamic outputs declared here will be forwarded to the underlying node
        so they are available when the workflow executes. Provide Python types
        such as str or list[int] for each output.

        Args:
            dynamic_outputs: Optional mapping from output names to Python types.
            **kwargs: Field values for the node.
        """

        outputs = {} if dynamic_outputs is None else dict(dynamic_outputs)
        super().__init__(dynamic_outputs=outputs, **kwargs)

    @property
    def out(self) -> DynamicOutputsProxy:
        return typing.cast(DynamicOutputsProxy, self._outputs_proxy())

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.generators.StructuredOutputGenerator

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
