# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.document
from nodetool.workflows.base_node import BaseNode


class ListDocuments(
    GraphNode[nodetool.nodes.nodetool.document.ListDocuments.OutputType]
):
    """

    List documents in a directory.
    files, list, directory
    """

    folder: str | OutputHandle[str] = connect_field(
        default="~", description="Directory to scan"
    )
    pattern: str | OutputHandle[str] = connect_field(
        default="*", description="File pattern to match (e.g. *.txt)"
    )
    recursive: bool | OutputHandle[bool] = connect_field(
        default=False, description="Search subdirectories"
    )

    @property
    def out(self) -> "ListDocumentsOutputs":
        return ListDocumentsOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.document.ListDocuments

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class ListDocumentsOutputs(OutputsProxy):
    @property
    def document(self) -> OutputHandle[types.DocumentRef]:
        return typing.cast(OutputHandle[types.DocumentRef], self["document"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.document
from nodetool.workflows.base_node import BaseNode


class LoadDocumentFile(
    SingleOutputGraphNode[types.DocumentRef], GraphNode[types.DocumentRef]
):
    """

    Read a document from disk.
    files, document, read, input, load, file
    """

    path: str | OutputHandle[str] = connect_field(
        default="", description="Path to the document to read"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.document.LoadDocumentFile

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.document
from nodetool.workflows.base_node import BaseNode


class SaveDocumentFile(SingleOutputGraphNode[typing.Any], GraphNode[typing.Any]):
    """

    Write a document to disk.
    files, document, write, output, save, file

    The filename can include time and date variables:
    %Y - Year, %m - Month, %d - Day
    %H - Hour, %M - Minute, %S - Second
    """

    document: types.DocumentRef | OutputHandle[types.DocumentRef] = connect_field(
        default=types.DocumentRef(type="document", uri="", asset_id=None, data=None),
        description="The document to save",
    )
    folder: str | OutputHandle[str] = connect_field(
        default="", description="Folder where the file will be saved"
    )
    filename: str | OutputHandle[str] = connect_field(
        default="",
        description="Name of the file to save. Supports strftime format codes.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.document.SaveDocumentFile

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.document
from nodetool.workflows.base_node import BaseNode


class SplitDocument(
    GraphNode[nodetool.nodes.nodetool.document.SplitDocument.OutputType]
):
    """

    Split text semantically.
    chroma, embedding, collection, RAG, index, text, markdown, semantic
    """

    embed_model: types.LanguageModel | OutputHandle[types.LanguageModel] = (
        connect_field(
            default=types.LanguageModel(
                type="language_model",
                provider=nodetool.metadata.types.Provider.Ollama,
                id="embeddinggemma",
                name="",
            ),
            description="Embedding model to use",
        )
    )
    document: types.DocumentRef | OutputHandle[types.DocumentRef] = connect_field(
        default=types.DocumentRef(type="document", uri="", asset_id=None, data=None),
        description="Document ID to associate with the text content",
    )
    buffer_size: int | OutputHandle[int] = connect_field(
        default=1, description="Buffer size for semantic splitting"
    )
    threshold: int | OutputHandle[int] = connect_field(
        default=95, description="Breakpoint percentile threshold for semantic splitting"
    )

    @property
    def out(self) -> "SplitDocumentOutputs":
        return SplitDocumentOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.document.SplitDocument

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class SplitDocumentOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def source_id(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["source_id"])

    @property
    def start_index(self) -> OutputHandle[int]:
        return typing.cast(OutputHandle[int], self["start_index"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.document
from nodetool.workflows.base_node import BaseNode


class SplitHTML(GraphNode[nodetool.nodes.nodetool.document.SplitHTML.OutputType]):
    """

    Split HTML content into semantic chunks based on HTML tags.
    html, text, semantic, tags, parsing
    """

    document: types.DocumentRef | OutputHandle[types.DocumentRef] = connect_field(
        default=types.DocumentRef(type="document", uri="", asset_id=None, data=None),
        description="Document ID to associate with the HTML content",
    )

    @property
    def out(self) -> "SplitHTMLOutputs":
        return SplitHTMLOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.document.SplitHTML

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class SplitHTMLOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def source_id(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["source_id"])

    @property
    def start_index(self) -> OutputHandle[int]:
        return typing.cast(OutputHandle[int], self["start_index"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.document
from nodetool.workflows.base_node import BaseNode


class SplitJSON(GraphNode[nodetool.nodes.nodetool.document.SplitJSON.OutputType]):
    """

    Split JSON content into semantic chunks.
    json, parsing, semantic, structured
    """

    document: types.DocumentRef | OutputHandle[types.DocumentRef] = connect_field(
        default=types.DocumentRef(type="document", uri="", asset_id=None, data=None),
        description="Document ID to associate with the JSON content",
    )
    include_metadata: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to include metadata in nodes"
    )
    include_prev_next_rel: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to include prev/next relationships"
    )

    @property
    def out(self) -> "SplitJSONOutputs":
        return SplitJSONOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.document.SplitJSON

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class SplitJSONOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def source_id(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["source_id"])

    @property
    def start_index(self) -> OutputHandle[int]:
        return typing.cast(OutputHandle[int], self["start_index"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.document
from nodetool.workflows.base_node import BaseNode


class SplitMarkdown(
    GraphNode[nodetool.nodes.nodetool.document.SplitMarkdown.OutputType]
):
    """

    Splits markdown text by headers while preserving header hierarchy in metadata.
    markdown, split, headers

    Use cases:
    - Splitting markdown documentation while preserving structure
    - Processing markdown files for semantic search
    - Creating context-aware chunks from markdown content
    """

    document: types.DocumentRef | OutputHandle[types.DocumentRef] = connect_field(
        default=types.DocumentRef(type="document", uri="", asset_id=None, data=None),
        description=None,
    )
    headers_to_split_on: list[tuple[str, str]] | OutputHandle[list[tuple[str, str]]] = (
        connect_field(
            default=[("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")],
            description="List of tuples containing (header_symbol, header_name)",
        )
    )
    strip_headers: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to remove headers from the output content"
    )
    return_each_line: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to split into individual lines instead of header sections",
    )
    chunk_size: int | OutputHandle[int] | None = connect_field(
        default=None, description="Optional maximum chunk size for further splitting"
    )
    chunk_overlap: int | OutputHandle[int] = connect_field(
        default=30, description="Overlap size when using chunk_size"
    )

    @property
    def out(self) -> "SplitMarkdownOutputs":
        return SplitMarkdownOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.document.SplitMarkdown

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class SplitMarkdownOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def source_id(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["source_id"])

    @property
    def start_index(self) -> OutputHandle[int]:
        return typing.cast(OutputHandle[int], self["start_index"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.document
from nodetool.workflows.base_node import BaseNode


class SplitRecursively(
    GraphNode[nodetool.nodes.nodetool.document.SplitRecursively.OutputType]
):
    """

    Splits text recursively using LangChain's RecursiveCharacterTextSplitter.
    text, split, chunks

    Use cases:
    - Splitting documents while preserving semantic relationships
    - Creating chunks for language model processing
    - Handling text in languages with/without word boundaries
    """

    document: types.DocumentRef | OutputHandle[types.DocumentRef] = connect_field(
        default=types.DocumentRef(type="document", uri="", asset_id=None, data=None),
        description=None,
    )
    chunk_size: int | OutputHandle[int] = connect_field(
        default=1000, description="Maximum size of each chunk in characters"
    )
    chunk_overlap: int | OutputHandle[int] = connect_field(
        default=200, description="Number of characters to overlap between chunks"
    )
    separators: list[str] | OutputHandle[list[str]] = connect_field(
        default=["\n\n", "\n", "."],
        description="List of separators to use for splitting, in order of preference",
    )

    @property
    def out(self) -> "SplitRecursivelyOutputs":
        return SplitRecursivelyOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.document.SplitRecursively

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class SplitRecursivelyOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def source_id(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["source_id"])

    @property
    def start_index(self) -> OutputHandle[int]:
        return typing.cast(OutputHandle[int], self["start_index"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.document
from nodetool.workflows.base_node import BaseNode


class SplitSentences(
    GraphNode[nodetool.nodes.nodetool.document.SplitSentences.OutputType]
):
    """

    Splits text into sentences using LangChain's SentenceTransformersTokenTextSplitter.
    sentences, split, nlp

    Use cases:
    - Natural sentence-based text splitting
    - Creating semantically meaningful chunks
    - Processing text for sentence-level analysis
    """

    document: types.DocumentRef | OutputHandle[types.DocumentRef] = connect_field(
        default=types.DocumentRef(type="document", uri="", asset_id=None, data=None),
        description=None,
    )
    chunk_size: int | OutputHandle[int] = connect_field(
        default=40, description="Maximum number of tokens per chunk"
    )
    chunk_overlap: int | OutputHandle[int] = connect_field(
        default=5, description="Number of tokens to overlap between chunks"
    )

    @property
    def out(self) -> "SplitSentencesOutputs":
        return SplitSentencesOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.nodetool.document.SplitSentences

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class SplitSentencesOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def source_id(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["source_id"])

    @property
    def start_index(self) -> OutputHandle[int]:
        return typing.cast(OutputHandle[int], self["start_index"])
