# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class AudioMixer(GraphNode[types.AudioRef]):
    """
    Mix up to 5 audio tracks together with individual volume controls.
    audio, mix, volume, combine, blend, layer, add, overlay

    Use cases:
    - Mix multiple audio tracks into a single output
    - Create layered soundscapes
    - Combine music, voice, and sound effects
    - Adjust individual track volumes
    """

    track1: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="First audio track to mix.",
    )
    track2: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="Second audio track to mix.",
    )
    track3: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="Third audio track to mix.",
    )
    track4: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="Fourth audio track to mix.",
    )
    track5: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="Fifth audio track to mix.",
    )
    volume1: float | OutputHandle[float] = connect_field(
        default=1.0, description="Volume for track 1. 1.0 is original volume."
    )
    volume2: float | OutputHandle[float] = connect_field(
        default=1.0, description="Volume for track 2. 1.0 is original volume."
    )
    volume3: float | OutputHandle[float] = connect_field(
        default=1.0, description="Volume for track 3. 1.0 is original volume."
    )
    volume4: float | OutputHandle[float] = connect_field(
        default=1.0, description="Volume for track 4. 1.0 is original volume."
    )
    volume5: float | OutputHandle[float] = connect_field(
        default=1.0, description="Volume for track 5. 1.0 is original volume."
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.AudioMixer"


AudioMixer.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class AudioToNumpy(GraphNode[nodetool.nodes.nodetool.audio.AudioToNumpy.OutputType]):
    """
    Convert audio to numpy array for processing.
    audio, numpy, convert, array

    Use cases:
    - Prepare audio for custom processing
    - Convert audio for machine learning models
    - Extract raw audio data for analysis
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The audio to convert to numpy.",
    )

    @property
    def out(self) -> "AudioToNumpyOutputs":
        return AudioToNumpyOutputs(self)

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.AudioToNumpy"


class AudioToNumpyOutputs(OutputsProxy):
    @property
    def array(self) -> OutputHandle[types.NPArray]:
        return typing.cast(OutputHandle[types.NPArray], self["array"])

    @property
    def sample_rate(self) -> OutputHandle[int]:
        return typing.cast(OutputHandle[int], self["sample_rate"])

    @property
    def channels(self) -> OutputHandle[int]:
        return typing.cast(OutputHandle[int], self["channels"])


AudioToNumpy.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class Concat(GraphNode[types.AudioRef]):
    """
    Concatenates two audio files together.
    audio, edit, join, +

    Use cases:
    - Combine multiple audio clips into a single file
    - Create longer audio tracks from shorter segments
    """

    a: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The first audio file.",
    )
    b: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The second audio file.",
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.Concat"


Concat.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class ConcatList(GraphNode[types.AudioRef]):
    """
    Concatenates multiple audio files together in sequence.
    audio, edit, join, multiple, +

    Use cases:
    - Combine multiple audio clips into a single file
    - Create longer audio tracks from multiple segments
    - Chain multiple audio files in order
    """

    audio_files: list[types.AudioRef] | OutputHandle[list[types.AudioRef]] = (
        connect_field(
            default=[], description="List of audio files to concatenate in sequence."
        )
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.ConcatList"


ConcatList.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class ConvertToArray(GraphNode[types.NPArray]):
    """
    Converts an audio file to a Array for further processing.
    audio, conversion, tensor

    Use cases:
    - Prepare audio data for machine learning models
    - Enable signal processing operations on audio
    - Convert audio to a format suitable for spectral analysisr
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The audio file to convert to a tensor.",
    )

    @property
    def output(self) -> OutputHandle[types.NPArray]:
        return typing.cast(OutputHandle[types.NPArray], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.ConvertToArray"


ConvertToArray.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class CreateSilence(GraphNode[types.AudioRef]):
    """
    Creates a silent audio file with a specified duration.
    audio, silence, empty

    Use cases:
    - Generate placeholder audio files
    - Create audio segments for padding or spacing
    - Add silence to the beginning or end of audio files
    """

    duration: float | OutputHandle[float] = connect_field(
        default=1.0, description="The duration of the silence in seconds."
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.CreateSilence"


CreateSilence.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class FadeIn(GraphNode[types.AudioRef]):
    """
    Applies a fade-in effect to the beginning of an audio file.
    audio, edit, transition

    Use cases:
    - Create smooth introductions to audio tracks
    - Gradually increase volume at the start of a clip
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The audio file to apply fade-in to.",
    )
    duration: float | OutputHandle[float] = connect_field(
        default=1.0, description="Duration of the fade-in effect in seconds."
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.FadeIn"


FadeIn.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class FadeOut(GraphNode[types.AudioRef]):
    """
    Applies a fade-out effect to the end of an audio file.
    audio, edit, transition

    Use cases:
    - Create smooth endings to audio tracks
    - Gradually decrease volume at the end of a clip
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The audio file to apply fade-out to.",
    )
    duration: float | OutputHandle[float] = connect_field(
        default=1.0, description="Duration of the fade-out effect in seconds."
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.FadeOut"


FadeOut.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class LoadAudioAssets(
    GraphNode[nodetool.nodes.nodetool.audio.LoadAudioAssets.OutputType]
):
    """
    Load audio files from an asset folder.
    load, audio, file, import
    """

    folder: types.FolderRef | OutputHandle[types.FolderRef] = connect_field(
        default=types.FolderRef(type="folder", uri="", asset_id=None, data=None),
        description="The asset folder to load the audio files from.",
    )

    @property
    def out(self) -> "LoadAudioAssetsOutputs":
        return LoadAudioAssetsOutputs(self)

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.LoadAudioAssets"


class LoadAudioAssetsOutputs(OutputsProxy):
    @property
    def audio(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self["audio"])

    @property
    def name(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["name"])


LoadAudioAssets.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class LoadAudioFile(GraphNode[types.AudioRef]):
    """
    Read an audio file from disk.
    audio, input, load, file

    Use cases:
    - Load audio for processing
    - Import sound files for editing
    - Read audio assets for a workflow
    """

    path: str | OutputHandle[str] = connect_field(
        default="", description="Path to the audio file to read"
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.LoadAudioFile"


LoadAudioFile.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class LoadAudioFolder(
    GraphNode[nodetool.nodes.nodetool.audio.LoadAudioFolder.OutputType]
):
    """
    Load all audio files from a folder, optionally including subfolders.
    audio, load, folder, files

    Use cases:
    - Batch import audio for processing
    - Build datasets from a directory tree
    - Iterate over audio collections
    """

    folder: str | OutputHandle[str] = connect_field(
        default="", description="Folder to scan for audio files"
    )
    include_subdirectories: bool | OutputHandle[bool] = connect_field(
        default=False, description="Include audio in subfolders"
    )
    extensions: list[str] | OutputHandle[list[str]] = connect_field(
        default=[".mp3", ".wav", ".flac", ".ogg", ".m4a", ".aac"],
        description="Audio file extensions to include",
    )

    @property
    def out(self) -> "LoadAudioFolderOutputs":
        return LoadAudioFolderOutputs(self)

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.LoadAudioFolder"


class LoadAudioFolderOutputs(OutputsProxy):
    @property
    def audio(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self["audio"])

    @property
    def path(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["path"])


LoadAudioFolder.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class MonoToStereo(GraphNode[types.AudioRef]):
    """
    Converts a mono audio signal to stereo.
    audio, convert, channels

    Use cases:
    - Expand mono recordings for stereo playback systems
    - Prepare audio for further stereo processing
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The mono audio file to convert.",
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.MonoToStereo"


MonoToStereo.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class Normalize(GraphNode[types.AudioRef]):
    """
    Normalizes the volume of an audio file.
    audio, fix, dynamics, volume

    Use cases:
    - Ensure consistent volume across multiple audio files
    - Adjust overall volume level before further processing
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The audio file to normalize.",
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.Normalize"


Normalize.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class NumpyToAudio(GraphNode[types.AudioRef]):
    """
    Convert numpy array to audio.
    audio, numpy, convert

    Use cases:
    - Convert processed audio data back to audio format
    - Create audio from machine learning model outputs
    - Generate audio from synthesized waveforms
    """

    array: types.NPArray | OutputHandle[types.NPArray] = connect_field(
        default=types.NPArray(type="np_array", value=None, dtype="<i8", shape=(1,)),
        description="The numpy array to convert to audio.",
    )
    sample_rate: int | OutputHandle[int] = connect_field(
        default=44100, description="Sample rate in Hz."
    )
    channels: int | OutputHandle[int] = connect_field(
        default=1, description="Number of audio channels (1 or 2)."
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.NumpyToAudio"


NumpyToAudio.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class OverlayAudio(GraphNode[types.AudioRef]):
    """
    Overlays two audio files together.
    audio, edit, transform

    Use cases:
    - Mix background music with voice recording
    - Layer sound effects over an existing audio track
    """

    a: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The first audio file.",
    )
    b: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The second audio file.",
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.OverlayAudio"


OverlayAudio.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio
import nodetool.nodes.nodetool.audio
import nodetool.nodes.nodetool.audio


class RealtimeWhisper(
    GraphNode[nodetool.nodes.nodetool.audio.RealtimeWhisper.OutputType]
):
    """
    Stream audio input to WhisperLive and emit real-time transcription.
    realtime, whisper, transcription, streaming, audio-to-text, speech-to-text

    Emits:
      - `chunk` Chunk(content=..., done=False) for transcript deltas
      - `chunk` Chunk(content="", done=True) to mark segment end
      - `text` final aggregated transcript when input ends
    """

    WhisperModel: typing.ClassVar[type] = (
        nodetool.nodes.nodetool.audio.RealtimeWhisper.WhisperModel
    )
    Language: typing.ClassVar[type] = (
        nodetool.nodes.nodetool.audio.RealtimeWhisper.Language
    )
    model: nodetool.nodes.nodetool.audio.RealtimeWhisper.WhisperModel = Field(
        default=nodetool.nodes.nodetool.audio.RealtimeWhisper.WhisperModel.TINY,
        description="Whisper model size - larger models are more accurate but slower",
    )
    language: nodetool.nodes.nodetool.audio.RealtimeWhisper.Language = Field(
        default=nodetool.nodes.nodetool.audio.RealtimeWhisper.Language.ENGLISH,
        description="Language code for transcription, or 'auto' for automatic detection",
    )
    chunk: types.Chunk | OutputHandle[types.Chunk] = connect_field(
        default=types.Chunk(
            type="chunk",
            node_id=None,
            content_type="text",
            content="",
            content_metadata={},
            done=False,
        ),
        description="The audio chunk to transcribe",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.0, description="Sampling temperature for transcription"
    )
    initial_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Optional initial prompt to guide transcription style"
    )

    @property
    def out(self) -> "RealtimeWhisperOutputs":
        return RealtimeWhisperOutputs(self)

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.RealtimeWhisper"


class RealtimeWhisperOutputs(OutputsProxy):
    @property
    def start(self) -> OutputHandle[float]:
        return typing.cast(OutputHandle[float], self["start"])

    @property
    def end(self) -> OutputHandle[float]:
        return typing.cast(OutputHandle[float], self["end"])

    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def chunk(self) -> OutputHandle[types.Chunk]:
        return typing.cast(OutputHandle[types.Chunk], self["chunk"])

    @property
    def speaker(self) -> OutputHandle[int]:
        return typing.cast(OutputHandle[int], self["speaker"])

    @property
    def detected_language(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["detected_language"])

    @property
    def translation(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["translation"])


RealtimeWhisper.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class RemoveSilence(GraphNode[types.AudioRef]):
    """
    Removes or shortens silence in an audio file with smooth transitions.
    audio, edit, clean

    Use cases:
    - Trim silent parts from beginning/end of recordings
    - Remove or shorten long pauses between speech segments
    - Apply crossfade for smooth transitions
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The audio file to process.",
    )
    min_length: int | OutputHandle[int] = connect_field(
        default=200,
        description="Minimum length of silence to be processed (in milliseconds).",
    )
    threshold: int | OutputHandle[int] = connect_field(
        default=-40,
        description="Silence threshold in dB (relative to full scale). Higher values detect more silence.",
    )
    reduction_factor: float | OutputHandle[float] = connect_field(
        default=1.0,
        description="Factor to reduce silent parts (0.0 to 1.0). 0.0 keeps silence as is, 1.0 removes it completely.",
    )
    crossfade: int | OutputHandle[int] = connect_field(
        default=10,
        description="Duration of crossfade in milliseconds to apply between segments for smooth transitions.",
    )
    min_silence_between_parts: int | OutputHandle[int] = connect_field(
        default=100,
        description="Minimum silence duration in milliseconds to maintain between non-silent segments",
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.RemoveSilence"


RemoveSilence.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class Repeat(GraphNode[types.AudioRef]):
    """
    Loops an audio file a specified number of times.
    audio, edit, repeat

    Use cases:
    - Create repeating background sounds or music
    - Extend short audio clips to fill longer durations
    - Generate rhythmic patterns from short samples
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The audio file to loop.",
    )
    loops: int | OutputHandle[int] = connect_field(
        default=2,
        description="Number of times to loop the audio. Minimum 1 (plays once), maximum 100.",
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.Repeat"


Repeat.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class Reverse(GraphNode[types.AudioRef]):
    """
    Reverses an audio file.
    audio, edit, transform

    Use cases:
    - Create reverse audio effects
    - Generate backwards speech or music
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The audio file to reverse.",
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.Reverse"


Reverse.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class SaveAudio(GraphNode[types.AudioRef]):
    """
    Save an audio file to a specified asset folder.
    audio, folder, name

    Use cases:
    - Save generated audio files with timestamps
    - Organize outputs into specific folders
    - Create backups of generated audio
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description=None,
    )
    folder: types.FolderRef | OutputHandle[types.FolderRef] = connect_field(
        default=types.FolderRef(type="folder", uri="", asset_id=None, data=None),
        description="The asset folder to save the audio file to. ",
    )
    name: str | OutputHandle[str] = connect_field(
        default="%Y-%m-%d-%H-%M-%S.opus",
        description="\n        The name of the audio file.\n        You can use time and date variables to create unique names:\n        %Y - Year\n        %m - Month\n        %d - Day\n        %H - Hour\n        %M - Minute\n        %S - Second\n        ",
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.SaveAudio"


SaveAudio.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class SaveAudioFile(GraphNode[types.AudioRef]):
    """
    Write an audio file to disk.
    audio, output, save, file

    The filename can include time and date variables:
    %Y - Year, %m - Month, %d - Day
    %H - Hour, %M - Minute, %S - Second
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The audio to save",
    )
    folder: str | OutputHandle[str] = connect_field(
        default="", description="Folder where the file will be saved"
    )
    filename: str | OutputHandle[str] = connect_field(
        default="",
        description="\n        Name of the file to save.\n        You can use time and date variables to create unique names:\n        %Y - Year\n        %m - Month\n        %d - Day\n        %H - Hour\n        %M - Minute\n        %S - Second\n        ",
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.SaveAudioFile"


SaveAudioFile.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class SliceAudio(GraphNode[types.AudioRef]):
    """
    Extracts a section of an audio file.
    audio, edit, trim

    Use cases:
    - Cut out a specific clip from a longer audio file
    - Remove unwanted portions from beginning or end
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The audio file.",
    )
    start: float | OutputHandle[float] = connect_field(
        default=0.0, description="The start time in seconds."
    )
    end: float | OutputHandle[float] = connect_field(
        default=1.0, description="The end time in seconds."
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.SliceAudio"


SliceAudio.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class StereoToMono(GraphNode[types.AudioRef]):
    """
    Converts a stereo audio signal to mono.
    audio, convert, channels

    Use cases:
    - Reduce file size for mono-only applications
    - Simplify audio for certain processing tasks
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The stereo audio file to convert.",
    )
    method: str | OutputHandle[str] = connect_field(
        default="average",
        description="Method to use for conversion: 'average', 'left', or 'right'.",
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.StereoToMono"


StereoToMono.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class TextToSpeech(GraphNode[nodetool.nodes.nodetool.audio.TextToSpeech.OutputType]):
    """
    Generate speech audio from text using any supported TTS provider.
    Automatically routes to the appropriate backend (OpenAI, HuggingFace, MLX).
    audio, generation, AI, text-to-speech, tts, voice

    Use cases:
    - Create voiceovers for videos and presentations
    - Generate natural-sounding narration for content
    - Build voice assistants and chatbots
    - Convert written content to audio format
    - Create accessible audio versions of text
    """

    model: types.TTSModel | OutputHandle[types.TTSModel] = connect_field(
        default=types.TTSModel(
            type="tts_model",
            provider=nodetool.metadata.types.Provider.OpenAI,
            id="tts-1",
            name="TTS 1",
            voices=["alloy", "echo", "fable", "onyx", "nova", "shimmer"],
            selected_voice="",
        ),
        description="The text-to-speech model to use",
    )
    text: str | OutputHandle[str] = connect_field(
        default="Hello! This is a text-to-speech demonstration.",
        description="Text to convert to speech",
    )
    speed: float | OutputHandle[float] = connect_field(
        default=1.0, description="Speech speed multiplier (0.25 to 4.0)"
    )

    @property
    def out(self) -> "TextToSpeechOutputs":
        return TextToSpeechOutputs(self)

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.TextToSpeech"


class TextToSpeechOutputs(OutputsProxy):
    @property
    def audio(self) -> OutputHandle[nodetool.metadata.types.AudioRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.AudioRef], self["audio"]
        )

    @property
    def chunk(self) -> OutputHandle[types.Chunk]:
        return typing.cast(OutputHandle[types.Chunk], self["chunk"])


TextToSpeech.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.audio


class Trim(GraphNode[types.AudioRef]):
    """
    Trim an audio file to a specified duration.
    audio, trim, cut

    Use cases:
    - Remove silence from the beginning or end of audio files
    - Extract specific segments from audio files
    - Prepare audio data for machine learning models
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The audio file to trim.",
    )
    start: float | OutputHandle[float] = connect_field(
        default=0.0, description="The start time of the trimmed audio in seconds."
    )
    end: float | OutputHandle[float] = connect_field(
        default=0.0, description="The end time of the trimmed audio in seconds."
    )

    @property
    def output(self) -> OutputHandle[types.AudioRef]:
        return typing.cast(OutputHandle[types.AudioRef], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.audio.Trim"


Trim.model_rebuild(force=True)
