# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class AddColumn(GraphNode[types.DataframeRef]):
    """
    Add list of values as new column to dataframe.
    dataframe, column, list

    Use cases:
    - Incorporate external data into existing dataframe
    - Add calculated results as new column
    - Augment dataframe with additional features
    """

    dataframe: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="Dataframe object to add a new column to.",
    )
    column_name: str | OutputHandle[str] = connect_field(
        default="",
        description="The name of the new column to be added to the dataframe.",
    )
    values: list[Any] | OutputHandle[list[Any]] = connect_field(
        default=[],
        description="A list of any type of elements which will be the new column's values.",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.AddColumn"


AddColumn.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class Aggregate(GraphNode[types.DataframeRef]):
    """
    Aggregate dataframe by one or more columns.
    aggregate, groupby, group, sum, mean, count, min, max, std, var, median, first, last

    Use cases:
    - Prepare data for aggregation operations
    - Analyze data by categories
    - Create summary statistics by groups
    """

    dataframe: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The DataFrame to group.",
    )
    columns: str | OutputHandle[str] = connect_field(
        default="", description="Comma-separated column names to group by."
    )
    aggregation: str | OutputHandle[str] = connect_field(
        default="sum",
        description="Aggregation function: sum, mean, count, min, max, std, var, median, first, last",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.Aggregate"


Aggregate.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class Append(GraphNode[types.DataframeRef]):
    """
    Append two dataframes along rows.
    append, concat, rows

    Use cases:
    - Combine data from multiple time periods
    - Merge datasets with same structure
    - Aggregate data from different sources
    """

    dataframe_a: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="First DataFrame to be appended.",
    )
    dataframe_b: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="Second DataFrame to be appended.",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.Append"


Append.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class DropDuplicates(GraphNode[types.DataframeRef]):
    """
    Remove duplicate rows from dataframe.
    duplicates, unique, clean

    Use cases:
    - Clean dataset by removing redundant entries
    - Ensure data integrity in analysis
    - Prepare data for unique value operations
    """

    df: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The input DataFrame.",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.DropDuplicates"


DropDuplicates.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class DropNA(GraphNode[types.DataframeRef]):
    """
    Remove rows with NA values from dataframe.
    na, missing, clean

    Use cases:
    - Clean dataset by removing incomplete entries
    - Prepare data for analysis requiring complete cases
    - Improve data quality for modeling
    """

    df: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The input DataFrame.",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.DropNA"


DropNA.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class ExtractColumn(GraphNode[list[Any]]):
    """
    Convert dataframe column to list.
    dataframe, column, list

    Use cases:
    - Extract data for use in other processing steps
    - Prepare column data for plotting or analysis
    - Convert categorical data to list for encoding
    """

    dataframe: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The input dataframe.",
    )
    column_name: str | OutputHandle[str] = connect_field(
        default="", description="The name of the column to be converted to a list."
    )

    @property
    def output(self) -> OutputHandle[list[Any]]:
        return typing.cast(OutputHandle[list[Any]], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.ExtractColumn"


ExtractColumn.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class FillNA(GraphNode[types.DataframeRef]):
    """
    Fill missing values in dataframe.
    fillna, missing, impute

    Use cases:
    - Handle missing data
    - Prepare data for analysis
    - Improve data quality
    """

    dataframe: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The DataFrame with missing values.",
    )
    value: Any | OutputHandle[Any] = connect_field(
        default=0, description="Value to use for filling missing values."
    )
    method: str | OutputHandle[str] = connect_field(
        default="value",
        description="Method for filling: value, forward, backward, mean, median",
    )
    columns: str | OutputHandle[str] = connect_field(
        default="",
        description="Comma-separated column names to fill. Leave empty for all columns.",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.FillNA"


FillNA.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class Filter(GraphNode[types.DataframeRef]):
    """
    Filter dataframe based on condition.
    filter, query, condition

    Example conditions:
    age > 30
    age > 30 and salary < 50000
    name == 'John Doe'
    100 <= price <= 200
    status in ['Active', 'Pending']
    not (age < 18)

    Use cases:
    - Extract subset of data meeting specific criteria
    - Remove outliers or invalid data points
    - Focus analysis on relevant data segments
    """

    df: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The DataFrame to filter.",
    )
    condition: str | OutputHandle[str] = connect_field(
        default="",
        description="The filtering condition to be applied to the DataFrame, e.g. column_name > 5.",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.Filter"


Filter.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class FindRow(GraphNode[types.DataframeRef]):
    """
    Find the first row in a dataframe that matches a given condition.
    filter, query, condition, single row

    Example conditions:
    age > 30
    age > 30 and salary < 50000
    name == 'John Doe'
    100 <= price <= 200
    status in ['Active', 'Pending']
    not (age < 18)

    Use cases:
    - Retrieve specific record based on criteria
    - Find first occurrence of a particular condition
    - Extract single data point for further analysis
    """

    df: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The DataFrame to search.",
    )
    condition: str | OutputHandle[str] = connect_field(
        default="",
        description="The condition to filter the DataFrame, e.g. 'column_name == value'.",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.FindRow"


FindRow.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class FromList(GraphNode[types.DataframeRef]):
    """
    Convert list of dicts to dataframe.
    list, dataframe, convert

    Use cases:
    - Transform list data into structured dataframe
    - Prepare list data for analysis or visualization
    - Convert API responses to dataframe format
    """

    values: list[Any] | OutputHandle[list[Any]] = connect_field(
        default=[],
        description="List of values to be converted, each value will be a row.",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.FromList"


FromList.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class ImportCSV(GraphNode[types.DataframeRef]):
    """
    Convert CSV string to dataframe.
    csv, dataframe, import

    Use cases:
    - Import CSV data from string input
    - Convert CSV responses from APIs to dataframe
    """

    csv_data: str | OutputHandle[str] = connect_field(
        default="", description="String input of CSV formatted text."
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.ImportCSV"


ImportCSV.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class JSONToDataframe(GraphNode[types.DataframeRef]):
    """
    Transforms a JSON string into a pandas DataFrame.
    json, dataframe, conversion

    Use cases:
    - Converting API responses to tabular format
    - Preparing JSON data for analysis or visualization
    - Structuring unstructured JSON data for further processing
    """

    text: str | OutputHandle[str] = connect_field(default="", description=None)

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.JSONToDataframe"


JSONToDataframe.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class Join(GraphNode[types.DataframeRef]):
    """
    Join two dataframes on specified column.
    join, merge, column

    Use cases:
    - Combine data from related tables
    - Enrich dataset with additional information
    - Link data based on common identifiers
    """

    dataframe_a: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="First DataFrame to be merged.",
    )
    dataframe_b: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="Second DataFrame to be merged.",
    )
    join_on: str | OutputHandle[str] = connect_field(
        default="", description="The column name on which to join the two dataframes."
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.Join"


Join.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class LoadCSVAssets(GraphNode[nodetool.nodes.nodetool.data.LoadCSVAssets.OutputType]):
    """
    Load dataframes from an asset folder.
    load, dataframe, file, import

    Use cases:
    - Load multiple dataframes from a folder
    - Process multiple datasets in sequence
    - Batch import of data files
    """

    folder: types.FolderRef | OutputHandle[types.FolderRef] = connect_field(
        default=types.FolderRef(type="folder", uri="", asset_id=None, data=None),
        description="The asset folder to load the dataframes from.",
    )

    @property
    def out(self) -> "LoadCSVAssetsOutputs":
        return LoadCSVAssetsOutputs(self)

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.LoadCSVAssets"


class LoadCSVAssetsOutputs(OutputsProxy):
    @property
    def dataframe(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(OutputHandle[types.DataframeRef], self["dataframe"])

    @property
    def name(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["name"])


LoadCSVAssets.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class LoadCSVFile(GraphNode[types.DataframeRef]):
    """
    Load CSV file from file path.
    csv, dataframe, import
    """

    file_path: str | OutputHandle[str] = connect_field(
        default="", description="The path to the CSV file to load."
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.LoadCSVFile"


LoadCSVFile.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class LoadCSVURL(GraphNode[types.DataframeRef]):
    """
    Load CSV file from URL.
    csv, dataframe, import
    """

    url: str | OutputHandle[str] = connect_field(
        default="", description="The URL of the CSV file to load."
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.LoadCSVURL"


LoadCSVURL.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class Merge(GraphNode[types.DataframeRef]):
    """
    Merge two dataframes along columns.
    merge, concat, columns

    Use cases:
    - Combine data from multiple sources
    - Add new features to existing dataframe
    - Merge time series data from different periods
    """

    dataframe_a: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="First DataFrame to be merged.",
    )
    dataframe_b: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="Second DataFrame to be merged.",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.Merge"


Merge.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class Pivot(GraphNode[types.DataframeRef]):
    """
    Pivot dataframe to reshape data.
    pivot, reshape, transform

    Use cases:
    - Transform long data to wide format
    - Create cross-tabulation tables
    - Reorganize data for visualization
    """

    dataframe: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The DataFrame to pivot.",
    )
    index: str | OutputHandle[str] = connect_field(
        default="", description="Column name to use as index (rows)."
    )
    columns: str | OutputHandle[str] = connect_field(
        default="", description="Column name to use as columns."
    )
    values: str | OutputHandle[str] = connect_field(
        default="", description="Column name to use as values."
    )
    aggfunc: str | OutputHandle[str] = connect_field(
        default="sum",
        description="Aggregation function: sum, mean, count, min, max, first, last",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.Pivot"


Pivot.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class Rename(GraphNode[types.DataframeRef]):
    """
    Rename columns in dataframe.
    rename, columns, names

    Use cases:
    - Standardize column names
    - Make column names more descriptive
    - Prepare data for specific requirements
    """

    dataframe: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The DataFrame to rename columns.",
    )
    rename_map: str | OutputHandle[str] = connect_field(
        default="", description="Column rename mapping in format: old1:new1,old2:new2"
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.Rename"


Rename.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class RowIterator(GraphNode[nodetool.nodes.nodetool.data.RowIterator.OutputType]):
    """
    Iterate over rows of a dataframe.
    """

    dataframe: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The input dataframe.",
    )

    @property
    def out(self) -> "RowIteratorOutputs":
        return RowIteratorOutputs(self)

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.RowIterator"


class RowIteratorOutputs(OutputsProxy):
    @property
    def dict(self) -> OutputHandle[dict]:
        return typing.cast(OutputHandle[dict], self["dict"])

    @property
    def index(self) -> OutputHandle[Any]:
        return typing.cast(OutputHandle[Any], self["index"])


RowIterator.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class SaveCSVDataframeFile(GraphNode[types.DataframeRef]):
    """
    Write a pandas DataFrame to a CSV file.
    files, csv, write, output, save, file

    The filename can include time and date variables:
    %Y - Year, %m - Month, %d - Day
    %H - Hour, %M - Minute, %S - Second
    """

    dataframe: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="DataFrame to write to CSV",
    )
    folder: str | OutputHandle[str] = connect_field(
        default="", description="Folder where the file will be saved"
    )
    filename: str | OutputHandle[str] = connect_field(
        default="",
        description="Name of the CSV file to save. Supports strftime format codes.",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.SaveCSVDataframeFile"


SaveCSVDataframeFile.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class SaveDataframe(GraphNode[types.DataframeRef]):
    """
    Save dataframe in specified folder.
    csv, folder, save

    Use cases:
    - Export processed data for external use
    - Create backups of dataframes
    """

    df: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description=None,
    )
    folder: types.FolderRef | OutputHandle[types.FolderRef] = connect_field(
        default=types.FolderRef(type="folder", uri="", asset_id=None, data=None),
        description="Name of the output folder.",
    )
    name: str | OutputHandle[str] = connect_field(
        default="output.csv",
        description="\n        Name of the output file.\n        You can use time and date variables to create unique names:\n        %Y - Year\n        %m - Month\n        %d - Day\n        %H - Hour\n        %M - Minute\n        %S - Second\n        ",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.SaveDataframe"


SaveDataframe.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class Schema(GraphNode[types.RecordType]):
    """
    Define a schema for a dataframe.
    schema, dataframe, create
    """

    columns: types.RecordType | OutputHandle[types.RecordType] = connect_field(
        default=types.RecordType(type="record_type", columns=[]),
        description="The columns to use in the dataframe.",
    )

    @property
    def output(self) -> OutputHandle[types.RecordType]:
        return typing.cast(OutputHandle[types.RecordType], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.Schema"


Schema.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class SelectColumn(GraphNode[types.DataframeRef]):
    """
    Select specific columns from dataframe.
    dataframe, columns, filter

    Use cases:
    - Extract relevant features for analysis
    - Reduce dataframe size by removing unnecessary columns
    - Prepare data for specific visualizations or models
    """

    dataframe: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="a dataframe from which columns are to be selected",
    )
    columns: str | OutputHandle[str] = connect_field(
        default="", description="comma separated list of column names"
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.SelectColumn"


SelectColumn.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class Slice(GraphNode[types.DataframeRef]):
    """
    Slice a dataframe by rows using start and end indices.
    slice, subset, rows

    Use cases:
    - Extract a specific range of rows from a large dataset
    - Create training and testing subsets for machine learning
    - Analyze data in smaller chunks
    """

    dataframe: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The input dataframe to be sliced.",
    )
    start_index: int | OutputHandle[int] = connect_field(
        default=0, description="The starting index of the slice (inclusive)."
    )
    end_index: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The ending index of the slice (exclusive). Use -1 for the last row.",
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.Slice"


Slice.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class SortByColumn(GraphNode[types.DataframeRef]):
    """
    Sort dataframe by specified column.
    sort, order, column

    Use cases:
    - Arrange data in ascending or descending order
    - Identify top or bottom values in dataset
    - Prepare data for rank-based analysis
    """

    df: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description=None,
    )
    column: str | OutputHandle[str] = connect_field(
        default="", description="The column to sort the DataFrame by."
    )

    @property
    def output(self) -> OutputHandle[types.DataframeRef]:
        return typing.cast(
            OutputHandle[types.DataframeRef], self._single_output_handle()
        )

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.SortByColumn"


SortByColumn.model_rebuild(force=True)


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.nodetool.data


class ToList(GraphNode[list[dict]]):
    """
    Convert dataframe to list of dictionaries.
    dataframe, list, convert

    Use cases:
    - Convert dataframe data for API consumption
    - Transform data for JSON serialization
    - Prepare data for document-based storage
    """

    dataframe: types.DataframeRef | OutputHandle[types.DataframeRef] = connect_field(
        default=types.DataframeRef(
            type="dataframe", uri="", asset_id=None, data=None, columns=None
        ),
        description="The input dataframe to convert.",
    )

    @property
    def output(self) -> OutputHandle[list[dict]]:
        return typing.cast(OutputHandle[list[dict]], self._single_output_handle())

    @classmethod
    def get_node_type(cls):
        return "nodetool.data.ToList"


ToList.model_rebuild(force=True)
