# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.openai.audio
from nodetool.workflows.base_node import BaseNode

class TextToSpeech(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

        Converts text to speech using OpenAI TTS models.
        audio, tts, text-to-speech, voice, synthesis

        Use cases:
        - Generate spoken content for videos or podcasts
        - Create voice-overs for presentations
        - Assist visually impaired users with text reading
        - Produce audio versions of written content
    """

    TtsModel: typing.ClassVar[type] = nodetool.nodes.openai.audio.TextToSpeech.TtsModel
    Voice: typing.ClassVar[type] = nodetool.nodes.openai.audio.TextToSpeech.Voice

    model: nodetool.nodes.openai.audio.TextToSpeech.TtsModel = Field(default=nodetool.nodes.openai.audio.TextToSpeech.TtsModel.tts_1, description=None)
    voice: nodetool.nodes.openai.audio.TextToSpeech.Voice = Field(default=nodetool.nodes.openai.audio.TextToSpeech.Voice.ALLOY, description=None)
    input: str | OutputHandle[str] = connect_field(default='', description=None)
    speed: float | OutputHandle[float] = connect_field(default=1.0, description=None)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.openai.audio.TextToSpeech

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.openai.audio
from nodetool.workflows.base_node import BaseNode

class Transcribe(GraphNode[nodetool.nodes.openai.audio.Transcribe.OutputType]):
    """

        Converts speech to text using OpenAI's speech-to-text API.
        audio, transcription, speech-to-text, stt, whisper

        Use cases:
        - Generate accurate transcriptions of audio content
        - Create searchable text from audio recordings
        - Support multiple languages for transcription
        - Enable automated subtitling and captioning
    """

    TranscriptionModel: typing.ClassVar[type] = nodetool.nodes.openai.audio.Transcribe.TranscriptionModel
    Language: typing.ClassVar[type] = nodetool.nodes.openai.audio.Transcribe.Language

    model: nodetool.nodes.openai.audio.Transcribe.TranscriptionModel = Field(default=nodetool.nodes.openai.audio.Transcribe.TranscriptionModel.WHISPER_1, description='The model to use for transcription.')
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='The audio file to transcribe (max 25 MB).')
    language: nodetool.nodes.openai.audio.Transcribe.Language = Field(default=nodetool.nodes.openai.audio.Transcribe.Language.NONE, description='The language of the input audio')
    timestamps: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to return timestamps for the generated text.')
    prompt: str | OutputHandle[str] = connect_field(default='', description="Optional text to guide the model's style or continue a previous audio segment.")
    temperature: float | OutputHandle[float] = connect_field(default=0, description='The sampling temperature between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.')

    @property
    def out(self) -> "TranscribeOutputs":
        return TranscribeOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.openai.audio.Transcribe

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()

class TranscribeOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self['text'])

    @property
    def words(self) -> OutputHandle[list[types.AudioChunk]]:
        return typing.cast(OutputHandle[list[types.AudioChunk]], self['words'])

    @property
    def segments(self) -> OutputHandle[list[types.AudioChunk]]:
        return typing.cast(OutputHandle[list[types.AudioChunk]], self['segments'])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.openai.audio
from nodetool.workflows.base_node import BaseNode

class Translate(SingleOutputGraphNode[str], GraphNode[str]):
    """

        Translates speech in audio to English text.
        audio, translation, speech-to-text, localization

        Use cases:
        - Translate foreign language audio content to English
        - Create English transcripts of multilingual recordings
        - Assist non-English speakers in understanding audio content
        - Enable cross-language communication in audio formats
    """

    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='The audio file to translate.')
    temperature: float | OutputHandle[float] = connect_field(default=0.0, description='The temperature to use for the translation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.openai.audio.Translate

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


